{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 初始化 Jupyter 环境 & 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于在 Jupyter 中强制刷新参数\n",
    "%reset -f\n",
    "\n",
    "# 导入相关的包\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import ale_py\n",
    "import pygame\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms import v2\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 设置相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关功能\n",
    "is_training = 1                     # 是否进行训练\n",
    "is_evaluate = 1                     # 是否进行评估, 此时会渲染游戏画面\n",
    "need_record = 1                     # 是否开启录像, 前提是 is_evaluate=1 才有效, 不会渲染游戏画面\n",
    "\n",
    "# 日志等级\n",
    "log_level = \"INFO\"\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=log_level)\n",
    "\n",
    "# 环境信息\n",
    "env_id = \"ALE/Galaxian-v5\"          # 游戏环境名\n",
    "env_high = 210                      # 游戏画面高度\n",
    "env_weight = 160                    # 游戏画面宽度\n",
    "max_steps = 10000                   # 每个回合的最大步数\n",
    "render_mode = \"rgb_array\"           # 渲染模式，可选 \"human\"、\"rgb_array\" 等\n",
    "\n",
    "# DQN 算法参数\n",
    "gamma = 0.99                        # 折扣因子\n",
    "max_epsilon = 0.9                   # epsilon 的最大值\n",
    "min_epsilon = 0.05                  # epsilon 的最小值\n",
    "decay_rate = 0.001                  # epsilon 的衰减率\n",
    "net_target_sync_freq = 10           # 目标网络的同步频率\n",
    "replay_buffer_size = 100000         # 经验回放缓冲区的大小\n",
    "frame_stack = 2                     # 帧堆叠的数量\n",
    "dqn_type = \"Double DQN\"             # DQN 类型, 可选的值为 \"DQN\"、\"Double DQN\"\n",
    "\n",
    "# 训练参数\n",
    "num_train_episodes = 3000           # 训练的总回合数\n",
    "num_train_steps = 100               # 每个回合的训练步数\n",
    "lr = 1e-4                           # 学习率\n",
    "batch_size = 64                     # 批量大小\n",
    "\n",
    "# 评估参数\n",
    "num_eval_episodes = 10              # 评估的回合数\n",
    "reward_threshold = 1000             # 评估奖励阈值, 如果高于阈值时, 日志等级为 Success, 否则为 Warning\n",
    "\n",
    "# 保存策略\n",
    "save_dir = \"./Gym_ALE_Galaxian\"                      # 数据保存的目录\n",
    "save_freq = 100                                      # 模型保存的频率\n",
    "max_checkpoints = 5                                  # 最大保存的模型数量\n",
    "checkpoint_perfix = \"DQN_Gym_ALE_Galaxian_\"          # 模型保存的前缀\n",
    "evaluate_record_perfix = \"DQN_Gym_ALE_Galaxian_\"     # 评估记录保存的前缀\n",
    "evaluate_record_fps = 30                             # 评估记录保存的帧率\n",
    "evaluate_record_quality = 10                         # 评估记录保存的质量, 值为 0 ~ 10\n",
    "\n",
    "# 其余参数初始化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gym.register_envs(ale_py)                            # Arcade Learning Environment(ALE) 环境需要提前注册"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 预处理函数 & 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_checkpoint_id(save_dir=save_dir, checkpoint_perfix=checkpoint_perfix):\n",
    "    \"\"\"\n",
    "    获取最新的模型路径, 并返回 \"模型路径\" 和 checkpoint 对应的 id\n",
    "    \"\"\"\n",
    "    # 如果指定目录不存在, 则直接创建该目录\n",
    "    if not Path(save_dir).exists():\n",
    "        Path(save_dir).mkdir(parents=True)\n",
    "        logger.debug(\"The specified directory does not exist, will create this folder\")\n",
    "        return None\n",
    "    \n",
    "    # 获取所有的模型文件\n",
    "    checkpoints = []\n",
    "    current_path = Path(save_dir)\n",
    "    for entry in current_path.iterdir():\n",
    "        if entry.is_file() and entry.suffix == \".pth\":\n",
    "            id = entry.name.split(checkpoint_perfix)[-1].split(\".\")[0]\n",
    "            checkpoints.append(id)\n",
    "    \n",
    "    # 寻找最大的 checkpoint id\n",
    "    if checkpoints.__len__() == 0:\n",
    "        logger.info(\"Not found any checkpoint files, will random initialization of network parameters\")\n",
    "        return None\n",
    "    else:\n",
    "        max_checkpoint_id = max(checkpoints)\n",
    "        max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{max_checkpoint_id}.pth\")\n",
    "        logger.info(f\"Found max checkpoints, max_checkpoint_id is {max_checkpoint_id}\")\n",
    "        return {\"max_checkpoint_path\" : max_checkpoint_path, \"max_checkpoint_id\" : max_checkpoint_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_old_checkpoint(save_dir=save_dir, checkpoint_perfix=checkpoint_perfix, max_checkpoints=max_checkpoints):\n",
    "    \"\"\"\n",
    "    删除旧的模型文件, 只保留最新的 max_checkpoints 个模型文件\n",
    "    \"\"\"\n",
    "    if Path(save_dir).exists():\n",
    "        checkpoints = []\n",
    "        for entry in Path(save_dir).iterdir():\n",
    "            if entry.is_file() and entry.suffix == \".pth\":\n",
    "                id = int(entry.name.split(checkpoint_perfix)[-1].split(\".\")[0])\n",
    "                checkpoints.append(id)\n",
    "    \n",
    "    if checkpoints.__len__() > max_checkpoints:\n",
    "        min_checkpoint_id = min(checkpoints)\n",
    "        min_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{min_checkpoint_id}.pth\")\n",
    "        os.remove(min_checkpoint_path)\n",
    "        logger.warning(f\"Delete old checkpoint file {min_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 定义智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    \"\"\"\n",
    "    智能体类, 封装了智能体所需要的各种方法\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size):\n",
    "        # Global Args\n",
    "        self.max_checkpoint = get_max_checkpoint_id()\n",
    "        self.current_epsilon = 1.0\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "\n",
    "        # Init DQN Network\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=frame_stack, out_channels=frame_stack * 2, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(in_channels=frame_stack * 2, out_channels=frame_stack * 4, kernel_size=3, stride=2, padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.LazyLinear(out_features=1024),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=1024),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=action_size),\n",
    "        )\n",
    "        if self.max_checkpoint is not None:\n",
    "            self.network.load_state_dict(torch.load(self.max_checkpoint[\"max_checkpoint_path\"]))\n",
    "        \n",
    "        # Init DQN Target Network\n",
    "        self.target_network = copy.deepcopy(self.network)\n",
    "\n",
    "        # Move to designated device\n",
    "        self.network.to(device)\n",
    "        self.target_network.to(device)\n",
    "\n",
    "        # Transfoms\n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Grayscale(1),\n",
    "        ])\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.network.parameters(), lr=lr)\n",
    "    \n",
    "    def sync_target_network(self):\n",
    "        \"\"\"\n",
    "        同步目标网络\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        logger.info(\"The target network has been synchronized\")\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        选择动作\n",
    "        \"\"\"\n",
    "        state = state.to(device)\n",
    "        action = self.network(state).argmax(dim=1).item()\n",
    "        return action\n",
    "    \n",
    "    def processing_states(self, frame_buffer):\n",
    "        \"\"\"\n",
    "        对输入的 frame_buffer 进行预处理, 并返回模型可以处理的 Tensor 对象\n",
    "        \"\"\"\n",
    "        # 将形状处理为 [batch_size, color_channel * stack_size, height, width]\n",
    "        states = torch.stack(tuple(self.transform(frame_buffer)), dim=0)\n",
    "        states = states.reshape(1, frame_stack, env_high, env_weight)\n",
    "        logger.debug(f\"Processing states shape: {states.shape}\")\n",
    "        return states\n",
    "    \n",
    "    def save_model(self, episodes):\n",
    "        \"\"\"\n",
    "        保存模型到指定路径, 并根据实际情况删除老的模型\n",
    "        \"\"\"\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint is None:\n",
    "            max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{episodes + int(self.max_checkpoint[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        torch.save(self.network.state_dict(), max_checkpoint_path)\n",
    "        logger.info(f\"Model saved to {max_checkpoint_path}\")\n",
    "        # 删掉老模型\n",
    "        del_old_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 调整环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定制环境\n",
    "class AleCustomEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    ALE 定制环境, 继承自 gym.Wrapper 类\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 调用原始环境的 step 方法\n",
    "        # 如果想要调整 env 的行为或奖励机制, 可以在这里进行调整\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 训练智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    # 训练用的主环境\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = AleCustomEnv(env)\n",
    "\n",
    "    # 实例化智能体 (动作空间必须是离散的)\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_size = env.action_space.n\n",
    "        Agent = RLAgent(action_size=action_size)\n",
    "    else:\n",
    "        logger.error(\"Action space is not Discrete!\")\n",
    "        raise ValueError(\"Action space is not Discrete!\")\n",
    "\n",
    "    # 循环每个回合\n",
    "    for episode in tqdm(range(num_train_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        current_action = None\n",
    "\n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "        \n",
    "        # 计算当前的 epsilon 值\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        logger.info(f\"Episode {episode + 1}, Current Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "        # 采样阶段: 将回合的每一步都保存到 \"经验回放缓冲区\" 中\n",
    "        for step in range(max_steps):\n",
    "            # 处理当前状态\n",
    "            current_states = Agent.processing_states(frame_buffer)\n",
    "            # 选择动作\n",
    "            if step % frame_stack == 0:\n",
    "                # 生成随机数\n",
    "                random_num = random.uniform(0, 1)\n",
    "                if random_num > epsilon:\n",
    "                    action = Agent.select_action(current_states)\n",
    "                    current_action = action\n",
    "                    logger.debug(f\"Selected action: {action}\")\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                    current_action = action\n",
    "                    logger.debug(f\"Random action: {action}\")\n",
    "            \n",
    "            # 执行动作\n",
    "            observation, reward, terminated, truncated, info = env.step(current_action)\n",
    "            total_reward += reward\n",
    "            logger.debug(f\"Step {step + 1} | Reward: {reward} | Total Reward: {total_reward} | Terminated: {terminated} | Truncated: {truncated} | Info: {info}\")\n",
    "\n",
    "            # 更新帧缓冲区\n",
    "            frame_buffer.append(observation)\n",
    "\n",
    "            # 保存到 \"经验回放缓冲区\"\n",
    "            Agent.replay_buffer.append({\"St\": current_states, \"At\": current_action, \"Rt\": reward, \"St+1\": Agent.processing_states(frame_buffer), \"Done\": terminated or truncated})\n",
    "\n",
    "            # 判断是否结束该回合\n",
    "            if terminated or truncated:\n",
    "                if total_reward > reward_threshold:\n",
    "                    logger.success(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                total_reward = 0\n",
    "                break\n",
    "        \n",
    "        # 训练阶段\n",
    "        for step in range(num_train_steps):\n",
    "            loss_avg = 0\n",
    "            loss_num = 0\n",
    "            # 加载数据\n",
    "            if len(Agent.replay_buffer) > batch_size:\n",
    "                mini_batch = random.sample(Agent.replay_buffer, batch_size)\n",
    "            else:\n",
    "                mini_batch = Agent.replay_buffer\n",
    "            # size: [bs, frame_stack, height, width]\n",
    "            batch_state = torch.cat([data[\"St\"] for data in mini_batch], dim=0).to(device)\n",
    "            # size: [bs, 1] -> 目的是为了与 gather 操作相兼容\n",
    "            batch_action = torch.tensor([data[\"At\"] for data in mini_batch]).unsqueeze(1).to(device)\n",
    "            # size: [bs]\n",
    "            batch_reward = torch.tensor([data[\"Rt\"] for data in mini_batch]).to(device)\n",
    "            # size: [bs, frame_stack, height, width]\n",
    "            batch_next_s = torch.cat([data[\"St+1\"] for data in mini_batch], dim=0).to(device)\n",
    "            # size: [bs]\n",
    "            batch_done = torch.tensor([data[\"Done\"] for data in mini_batch]).float().to(device)\n",
    "\n",
    "            if dqn_type == \"DQN\":\n",
    "                # 计算当前 action 的 Q 值\n",
    "                # gather 操作要求 batch_action 具有形状 [bs, 1], 因此需要 unsqueeze(1) 添加额外的维度, 然后用 squeeze(1) 去掉多余的维度\n",
    "                q_values = Agent.network(batch_state).gather(1, batch_action).squeeze(1)\n",
    "                # 计算 target 的值, 即利用 target network 来估计下一个状态的最大 Q 值\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = Agent.target_network(batch_next_s).max(1)[0]\n",
    "                    # 如果被终结, 则奖励只有 batch_reward\n",
    "                    target_q_values = batch_reward + gamma * next_q_values * (1 - batch_done)\n",
    "            elif dqn_type == \"Double DQN\":\n",
    "                # Double DQN 算法, 利用 Agent.network 来选择下一个状态的 action, 利用 Agent.target_network 来估计下一个状态的 Q 值\n",
    "                # 即将动作选择和 Q 值估计分离, 用来减轻 Q 估值过高的问题\n",
    "                q_values = Agent.network(batch_state).gather(1, batch_action).squeeze(1)\n",
    "                next_actions = Agent.network(batch_next_s).argmax(1)\n",
    "                next_q_values = Agent.target_network(batch_next_s).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "                target_q_values = batch_reward + gamma * next_q_values * (1 - batch_done)\n",
    "            \n",
    "            loss = Agent.loss_fn(q_values, target_q_values)\n",
    "            loss_value = loss.item()\n",
    "            loss_avg += loss_value\n",
    "            loss_num += 1\n",
    "            loss.backward()\n",
    "            Agent.optimizer.step()\n",
    "            Agent.optimizer.zero_grad()\n",
    "\n",
    "        logger.info(f\"Reploy_Buffer size: {len(Agent.replay_buffer)}; Loss Avg: {loss_avg/loss_num:.4f}\")\n",
    "\n",
    "        # 同步网络\n",
    "        if (episode + 1) % net_target_sync_freq == 0 and episode != 0:\n",
    "            Agent.sync_target_network()\n",
    "        \n",
    "        # 保存模型\n",
    "        if (episode + 1) % save_freq == 0 and episode != 0:\n",
    "            episodes = episode + 1\n",
    "            Agent.save_model(episodes)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 评估智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估但不录制视频\n",
    "if is_evaluate == 1 and need_record == 0:\n",
    "    eval_env = gym.make(env_id, render_mode=\"human\")\n",
    "    eval_env = AleCustomEnv(eval_env)\n",
    "# 评估且需要录制视频\n",
    "elif is_evaluate == 1 and need_record == 1:\n",
    "    eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    eval_env = AleCustomEnv(eval_env)\n",
    "\n",
    "# 初始化用于评估的参数\n",
    "frame_record = []\n",
    "max_reward = 0\n",
    "\n",
    "# 实例化用于评估的智能体\n",
    "Agent = RLAgent(action_size=eval_env.action_space.n)\n",
    "\n",
    "# 每个回合\n",
    "for episode in tqdm(range(num_eval_episodes)):\n",
    "    # 初始化环境\n",
    "    state, info = eval_env.reset()\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    frame_buffer = deque(maxlen=frame_stack)\n",
    "    current_action = None\n",
    "    # 初始化帧缓冲区\n",
    "    for _ in range(frame_stack): \n",
    "        frame_buffer.append(state)\n",
    "        \n",
    "    # 回合中的每一步\n",
    "    for step in range(max_steps):\n",
    "        # 处理当前状态\n",
    "        current_states = Agent.processing_states(frame_buffer)\n",
    "        # 选择动作\n",
    "        if step % frame_stack == 0:\n",
    "            action = Agent.select_action(current_states)\n",
    "            current_action = action\n",
    "        # 执行该动作\n",
    "        observation, reward, terminated, truncated, info = eval_env.step(current_action)\n",
    "        total_reward += reward\n",
    "        # 更新帧缓冲区\n",
    "        frame_buffer.append(observation)\n",
    "        # 如果需要记录视频, 则缓存视频帧, 否则渲染画面\n",
    "        if need_record:\n",
    "            frame_record.append(observation)\n",
    "        else:\n",
    "            eval_env.render()\n",
    "            \n",
    "        # 判断是否结束\n",
    "        if terminated or truncated:\n",
    "            # 如果需要记录视频, 则保留最好的记录\n",
    "            if need_record and total_reward > max_reward:\n",
    "                np_frame_record = np.array(frame_record)\n",
    "                max_reward = total_reward\n",
    "                frame_record.clear()\n",
    "            # 评估奖励\n",
    "            if total_reward > reward_threshold:\n",
    "                logger.success(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "            else:\n",
    "                logger.warning(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "# 记录评估结果(只记录最好的奖励轮次)\n",
    "if need_record:\n",
    "    record_file = f\"{os.path.abspath(os.path.join(save_dir, evaluate_record_perfix))}{int(max_reward)}.mp4\"\n",
    "    imageio.mimsave(record_file, np_frame_record, fps=evaluate_record_fps, quality=evaluate_record_quality)\n",
    "    logger.info(f\"The best evaluation record is: {record_file}\")\n",
    "\n",
    "# 关闭环境\n",
    "eval_env.close()\n",
    "pygame.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
