{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 初始化 Jupyter 环境 & 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于在 Jupyter 中强制刷新参数\n",
    "%reset -f\n",
    "\n",
    "# 导入相关的包\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pygame\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions import Normal\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 设置相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关功能\n",
    "is_training = 0                     # 是否进行训练\n",
    "is_evaluate = 1                     # 是否进行评估, 此时会渲染游戏画面\n",
    "need_record = 1                     # 是否开启录像, 前提是 is_evaluate=1 才有效, 不会渲染游戏画面\n",
    "\n",
    "# 日志等级\n",
    "log_level = \"INFO\"\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=log_level)\n",
    "\n",
    "# 环境信息\n",
    "env_id = \"Pendulum-v1\"              # 游戏环境名\n",
    "max_steps = 200                     # 每个回合的最大步数\n",
    "render_mode = \"rgb_array\"           # 渲染模式，可选 \"human\"、\"rgb_array\" 等\n",
    "\n",
    "# PPO 算法参数\n",
    "memory_buffer_size = 10000          # 记忆缓存区大小\n",
    "frame_stack = 1                     # 帧堆叠的数量\n",
    "gamma = 0.98                        # 折扣因子, 控制未来奖励的重要性\n",
    "lmbda = 0.98                        # GAE 参数, 控制轨迹长度\n",
    "clip_eps = 0.2                      # PPO 截断的范围\n",
    "epoch = 10                          # 样本重复训练的次数\n",
    "\n",
    "# 训练参数\n",
    "num_train_episodes = 1000           # 训练的总回合数\n",
    "a_lr = 5e-4                         # 策略模型的学习率\n",
    "c_lr = 1e-3                         # 价值模型的学习率\n",
    "\n",
    "\n",
    "# 评估参数\n",
    "num_eval_episodes = 10              # 评估的回合数\n",
    "reward_threshold = -500             # 评估奖励阈值, 如果高于阈值时, 日志等级为 Success, 否则为 Warning\n",
    "eval_sample_action = True           # 评估时的动作是否基于概率来采样, True 则基于概率来选取动作, False 则直接选取最大概率\n",
    "\n",
    "\n",
    "# 保存策略\n",
    "save_dir = \"./Gym_Classic_Pendulum_PPO\"                          # 数据保存的目录\n",
    "save_freq = 100                                                  # 模型保存的频率\n",
    "max_checkpoints = 5                                              # 最大保存的模型数量\n",
    "checkpoint_perfix_A = \"CheckPoint_Gym_Classic_Pendulum_A_\"       # 模型保存的前缀 Actor\n",
    "checkpoint_perfix_C = \"CheckPoint_Gym_Classic_Pendulum_C_\"       # 模型保存的前缀 Critic\n",
    "evaluate_record_perfix = \"Video_Gym_Classic_Pendulum_\"           # 评估记录保存的前缀\n",
    "evaluate_record_fps = 30                                         # 评估记录保存的帧率\n",
    "evaluate_record_quality = 10                                     # 评估记录保存的质量, 值为 0 ~ 10\n",
    "\n",
    "# 其余参数初始化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.manual_seed(0)                                           # 随机手动初始化\n",
    "# torch.autograd.set_detect_anomaly(True)                        # 开启 PyTorch 自动微分异常检测, 用于调试 NaN 梯度问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 预处理函数 & 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_checkpoint_id(checkpoint_perfix, save_dir=save_dir):\n",
    "    \"\"\"\n",
    "    获取最新的模型路径, 并返回 \"模型路径\" 和 checkpoint 对应的 id\n",
    "    \"\"\"\n",
    "    # 如果指定目录不存在, 则直接创建该目录\n",
    "    if not Path(save_dir).exists():\n",
    "        Path(save_dir).mkdir(parents=True)\n",
    "        logger.debug(\"The specified directory does not exist, will create this folder\")\n",
    "        return None\n",
    "    \n",
    "    # 获取所有的模型文件\n",
    "    checkpoints = []\n",
    "    current_path = Path(save_dir)\n",
    "    for entry in current_path.iterdir():\n",
    "        if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "            id = entry.name.split(checkpoint_perfix)[-1].split(\".\")[0]\n",
    "            checkpoints.append(int(id))\n",
    "    \n",
    "    # 寻找最大的 checkpoint id\n",
    "    if checkpoints.__len__() == 0:\n",
    "        logger.info(f\"Not found any {checkpoint_perfix} files, will random initialization of network parameters\")\n",
    "        return None\n",
    "    else:\n",
    "        max_checkpoint_id = max(checkpoints)\n",
    "        max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{max_checkpoint_id}.pth\")\n",
    "        logger.info(f\"Found max checkpoints, max_checkpoint_id is {max_checkpoint_id}\")\n",
    "        return {\"max_checkpoint_path\" : max_checkpoint_path, \"max_checkpoint_id\" : max_checkpoint_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_old_checkpoint(checkpoint_perfix, save_dir=save_dir, max_checkpoints=max_checkpoints):\n",
    "    \"\"\"\n",
    "    删除旧的模型文件, 只保留最新的 max_checkpoints 个模型文件\n",
    "    \"\"\"\n",
    "    if Path(save_dir).exists():\n",
    "        checkpoints = []\n",
    "        for entry in Path(save_dir).iterdir():\n",
    "            if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "                id = int(entry.name.split(checkpoint_perfix)[-1].split(\".\")[0])\n",
    "                checkpoints.append(id)\n",
    "    \n",
    "    if checkpoints.__len__() > max_checkpoints:\n",
    "        min_checkpoint_id = min(checkpoints)\n",
    "        min_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{min_checkpoint_id}.pth\")\n",
    "        os.remove(min_checkpoint_path)\n",
    "        logger.warning(f\"Delete old checkpoint file {min_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 定义智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Continuous(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    PPO 的 Actor 网络, 由于是连续动作, 因此需要单独定义\n",
    "    \"\"\"\n",
    "    def __init__(self, action_dim, hidden_dim=128, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fc1 = torch.nn.LazyLinear(out_features=hidden_dim)\n",
    "        self.fc_mu = torch.nn.LazyLinear(out_features=action_dim)\n",
    "        self.fc_std = torch.nn.LazyLinear(out_features=action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 均值 (μ)\n",
    "        # 这里用 2.0 * 是将均值缩放到 2.0 ~ -2.0 之间\n",
    "        mu = 2.0 * torch.tanh(self.fc_mu(x))\n",
    "        # 标准差 (σ), 该结果必须 > 0\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        # 返回 均值 (μ) 和 标准差 (σ)\n",
    "        return {\"mu\": mu, \"std\": std}\n",
    "    \n",
    "\n",
    "class Critic_Continuous(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    PPO 的 Critic 网络\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fc1 = torch.nn.LazyLinear(out_features=hidden_dim)\n",
    "        self.fc2 = torch.nn.LazyLinear(out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    \"\"\"\n",
    "    智能体类, 封装了智能体所需要的各种方法\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size):\n",
    "        # Global Args\n",
    "        self.max_checkpoint_a = get_max_checkpoint_id(checkpoint_perfix=checkpoint_perfix_A)\n",
    "        self.max_checkpoint_c = get_max_checkpoint_id(checkpoint_perfix=checkpoint_perfix_C)\n",
    "        self.memory_buffer = deque(maxlen=memory_buffer_size)\n",
    "\n",
    "        # Init Actor Network\n",
    "        self.actor_network = Actor_Continuous(action_dim=action_size)\n",
    "        if self.max_checkpoint_a is not None:\n",
    "            self.actor_network.load_state_dict(torch.load(self.max_checkpoint_a[\"max_checkpoint_path\"]))\n",
    "\n",
    "        # Init Critic Network\n",
    "        self.critic_network = Critic_Continuous()\n",
    "        if self.max_checkpoint_c is not None:\n",
    "            self.critic_network.load_state_dict(torch.load(self.max_checkpoint_c[\"max_checkpoint_path\"]))\n",
    "\n",
    "        # Move to designated device\n",
    "        self.actor_network.to(device)\n",
    "        self.critic_network.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        self.a_optimizer = torch.optim.AdamW(self.actor_network.parameters(), lr=a_lr)\n",
    "        self.c_optimizer = torch.optim.AdamW(self.critic_network.parameters(), lr=c_lr)\n",
    "\n",
    "    def processing_states(self, frame_buffer):\n",
    "        \"\"\"\n",
    "        对输入的 frame_buffer 进行预处理, 并返回模型可以处理的 Tensor 对象\n",
    "        \"\"\"\n",
    "        # 将形状处理为 [1, size * frame_buffer_size]\n",
    "        states = torch.tensor(np.array(frame_buffer))\n",
    "        states = states.reshape(1, -1)\n",
    "        logger.debug(f\"Processing states shape: {states.shape}\")\n",
    "        return states\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        选择动作, 某些算法需要对模型的输出进行采样, 因此可以将 sample 设置为 True\n",
    "        :param state:    神经网络可以接收的输入形状: [batch_size, color_channel * stack_size, height, width]\n",
    "        :param sample:   动作是否是采样, 如果不是则直接选择概率最高的动作\n",
    "        \"\"\"\n",
    "        state = state.to(device)\n",
    "        output = self.actor_network(state)\n",
    "        mu, std = output[\"mu\"], output[\"std\"]\n",
    "        action_dist = Normal(mu, std)\n",
    "        action = action_dist.sample()\n",
    "        return {\"action\": action}\n",
    "\n",
    "    def compute_advantage(self, gamma, lmbda, td_err):\n",
    "        \"\"\"\n",
    "        广义优势估计 (Generalized Advantage Estimation, GAE)\n",
    "        Args:\n",
    "        - gamma:  折扣因子 (0 ~ 1), 控制未来奖励的重要性\n",
    "        - lmbda:  GAE 衰减参数 (0 ~ 1), 控制轨迹长度\n",
    "        - td_err: 时间差分误差 (TD Error) 的张量\n",
    "        \"\"\"\n",
    "        # 将 td_err 从计算图中卸载下来, 避免影响梯度传递(核心思想为对于常量, 最好都执行 detach 来避免影响梯度传递)\n",
    "        td_err = td_err.detach().numpy()\n",
    "        advantage_list = []\n",
    "        advantage = 0.0\n",
    "        # 逆序遍历 td_err\n",
    "        for delta in td_err[::-1]:\n",
    "            # 核心公式: A_t = γλA_{t+1} + δ_t\n",
    "            # gamma * lmbda 控制信息衰减的乘子 (γλ)\n",
    "            # delta 当前时间步的 td_err (δ_t)\n",
    "            advantage = gamma * lmbda * advantage + delta\n",
    "            advantage_list.append(advantage)\n",
    "        # 将计算结果逆序回原始顺序 (因为之前是逆序的)\n",
    "        advantage_list.reverse()\n",
    "        # 将列表转换为 np.array, 这样转换为 tensor 的速度会更快\n",
    "        advantages = np.array(advantage_list, dtype=np.float32)\n",
    "        return torch.from_numpy(advantages).reshape(-1, 1).to(device=device)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        PPO 算法更新\n",
    "        \"\"\"\n",
    "        num_mems = len(self.memory_buffer)\n",
    "        logger.debug(f\"memory buffer size: {num_mems}\")\n",
    "\n",
    "        # 这里不需要保留和计算梯度, 只作为常量来看待, 否则会影响梯度传播, 例如 old_log_probs 和 old_action_dists\n",
    "        # https://github.com/boyu-ai/Hands-on-RL/issues/96\n",
    "        # 要么在 with torch.no_grad() 下获取, 要么手动执行 detach() 方法主动分离计算图\n",
    "        with torch.no_grad():\n",
    "            # 提取对应的数据\n",
    "            # 注意, 这里要提前处理形状, 防止在计算时广播导致形状不对\n",
    "            state = torch.cat([data[\"St\"] for data in self.memory_buffer], dim=0).to(device)\n",
    "            action = torch.tensor([data[\"At\"] for data in self.memory_buffer]).unsqueeze(1).to(device)\n",
    "            reward = torch.tensor([data[\"Rt\"] for data in self.memory_buffer]).unsqueeze(1).to(device)\n",
    "            next_state = torch.cat([data[\"St+1\"] for data in self.memory_buffer], dim=0).to(device)\n",
    "            done = torch.tensor([data[\"Done\"] for data in self.memory_buffer]).float().unsqueeze(1).to(device)\n",
    "            logger.debug(f\"state shape: {state.shape}, action shape: {action.shape}, reward shape: {reward.shape}, next_state shape: {next_state.shape}, done shape: {done.shape}\")\n",
    "\n",
    "            # 对奖励进行归一化, 方便训练\n",
    "            # 动态归一化: (reward - mean) / std\n",
    "            reward = (reward + 8.0) / 8.0\n",
    "            # reward = (reward - reward.mean()) / reward.std()\n",
    "\n",
    "            # Critic 网络: TD目标 & TD误差\n",
    "            td_tgt = reward + gamma * self.critic_network(next_state) * (1 - done)\n",
    "            td_tgt = td_tgt.float() # 强制 td_tgt 为单精度浮点数, 默认为双精度浮点数(暂不确定具体原因)\n",
    "            td_err = td_tgt - self.critic_network(state)\n",
    "\n",
    "            # GAE 的结果\n",
    "            advantage = self.compute_advantage(gamma, lmbda, td_err.cpu()).to(device)\n",
    "            \n",
    "            # 动作概率\n",
    "            output = self.actor_network(state)\n",
    "            mu, std = output[\"mu\"], output[\"std\"]\n",
    "            old_action_dists = torch.distributions.Normal(mu, std)\n",
    "            old_log_probs = old_action_dists.log_prob(action)\n",
    "        \n",
    "        # 更新主循环\n",
    "        a_loss = 0\n",
    "        c_loss = 0\n",
    "        for _ in range(epoch):\n",
    "            # 获取当前的动作概率\n",
    "            output = self.actor_network(state)\n",
    "            mu, std = output[\"mu\"], output[\"std\"]\n",
    "            new_action_dists = torch.distributions.Normal(mu, std)\n",
    "            log_probs = new_action_dists.log_prob(action)\n",
    "            # 计算新老概率的差距\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-clip_eps, 1+clip_eps) * advantage\n",
    "            # 损失函数\n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))\n",
    "            critic_loss = torch.mean(F.mse_loss(self.critic_network(state), td_tgt.detach()))\n",
    "            # 执行优化\n",
    "            self.a_optimizer.zero_grad()\n",
    "            self.c_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            a_loss += actor_loss.item()\n",
    "            c_loss += critic_loss.item()\n",
    "            # 进行梯度裁剪, 增强稳定性 (降低 NaN 出现的概率)\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor_network.parameters(), max_norm=2.0)\n",
    "            torch.nn.utils.clip_grad_norm_(self.critic_network.parameters(), max_norm=2.0)\n",
    "            self.a_optimizer.step()\n",
    "            self.c_optimizer.step()\n",
    "        logger.info(f\"a loss: {a_loss / epoch:.6f}\")\n",
    "        logger.info(f\"c loss: {c_loss / epoch:.6f}\")\n",
    "\n",
    "        # 清空经验池中的数据\n",
    "        self.memory_buffer.clear()\n",
    "    \n",
    "    def save_model(self, episodes):\n",
    "        \"\"\"\n",
    "        保存模型到指定路径, 并根据实际情况删除老的模型\n",
    "        \"\"\"\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint_a is None:\n",
    "            max_checkpoint_path_a = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_A}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path_a = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_A}{episodes + int(self.max_checkpoint_a[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint_c is None:\n",
    "            max_checkpoint_path_c = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_C}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path_c = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_C}{episodes + int(self.max_checkpoint_c[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        # 保存模型参数\n",
    "        torch.save(self.actor_network.state_dict(), max_checkpoint_path_a)\n",
    "        torch.save(self.critic_network.state_dict(), max_checkpoint_path_c)\n",
    "        logger.info(f\"Actor Model saved to {max_checkpoint_path_a}\")\n",
    "        logger.info(f\"Critic Model saved to {max_checkpoint_path_c}\")\n",
    "\n",
    "        # 删掉老模型\n",
    "        del_old_checkpoint(checkpoint_perfix=checkpoint_perfix_A)\n",
    "        del_old_checkpoint(checkpoint_perfix=checkpoint_perfix_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 调整环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定制环境\n",
    "class CustomEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    定制环境, 继承自 gym.Wrapper 类, 用于修改环境的行为或奖励机制\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境\n",
    "        \"\"\"\n",
    "        # 重置观察结果\n",
    "        observation = self.env.reset()\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作, 并调整了env 的行为或奖励机制\n",
    "        \"\"\"\n",
    "        # 调用原始环境的 step 方法\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # 返回最终结果: observation, reward, terminated, truncated, info\n",
    "        return observation, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 训练智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    # 训练用的主环境\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = CustomEnv(env)\n",
    "\n",
    "    # 实例化智能体 (动作空间必须是连续的)\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        # 连续的动作空间可以通过 action_space.shape[0] 来获取维度数\n",
    "        action_size = env.action_space.shape[0]\n",
    "        Agent = RLAgent(action_size=action_size)\n",
    "    else:\n",
    "        logger.error(\"Action space is not Box!\")\n",
    "        raise ValueError(\"Action space is not Box!\")\n",
    "\n",
    "    # 循环每个回合\n",
    "    for episode in tqdm(range(num_train_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        current_action = None\n",
    "        current_states = None\n",
    "        next_states = None\n",
    "        \n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "\n",
    "        # 回合中的每一步\n",
    "        for step in range(max_steps):\n",
    "            # 处理当前状态\n",
    "            current_states = Agent.processing_states(frame_buffer)\n",
    "\n",
    "            # 选择动作 & 对数概率\n",
    "            if step % frame_stack == 0:\n",
    "                output = Agent.select_action(current_states)\n",
    "                action = output['action'].item()\n",
    "                current_action = action\n",
    "                logger.debug(f\"Selected action: {action}\")\n",
    "            # 执行当前动作: current_action & 更新帧缓冲区\n",
    "            observation, reward, terminated, truncated, info = env.step(np.array([current_action]))\n",
    "            total_reward += reward\n",
    "            frame_buffer.append(observation)\n",
    "            next_states = Agent.processing_states(frame_buffer)\n",
    "            logger.debug(f\"Step {step + 1} | Reward: {reward} | Total Reward: {total_reward} | Terminated: {terminated} | Truncated: {truncated} | Info: {info}\")\n",
    "\n",
    "            # 保存到记忆区: 如果该帧是决策帧, 则新建记忆区记录\n",
    "            if step % frame_stack == 0:\n",
    "                Agent.memory_buffer.append({\"St\": current_states, \"At\": current_action, \"Rt\": reward, \"St+1\": next_states, \"Done\": terminated})\n",
    "            # 如果该帧不是决策帧, 则调整 & 完善记忆区记录\n",
    "            else:\n",
    "                # 奖励叠加\n",
    "                Agent.memory_buffer[-1][\"Rt\"] += reward\n",
    "                # 将 St+1 替换为最新的状态\n",
    "                Agent.memory_buffer[-1][\"St+1\"] = next_states\n",
    "\n",
    "            # 判断是否结束该回合\n",
    "            if terminated or truncated:\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Episode finish, total step {step + 1} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Episode finish, total step {step + 1} | Total Reward: {total_reward}\")\n",
    "                total_reward = 0\n",
    "                Agent.memory_buffer[-1][\"Done\"] = terminated\n",
    "                break\n",
    "        \n",
    "        # 更新模型\n",
    "        Agent.update()\n",
    "        \n",
    "        # 保存模型\n",
    "        if (episode + 1) % save_freq == 0 and episode != 0:\n",
    "            episodes = episode + 1\n",
    "            Agent.save_model(episodes)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 评估智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估但不录制视频\n",
    "if is_evaluate == 1 and need_record == 0:\n",
    "    eval_env = gym.make(env_id, render_mode=\"human\")\n",
    "    eval_env = CustomEnv(eval_env)\n",
    "# 评估且需要录制视频\n",
    "elif is_evaluate == 1 and need_record == 1:\n",
    "    eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    eval_env = CustomEnv(eval_env)\n",
    "\n",
    "# 如果启用了评估\n",
    "if is_evaluate == 1:\n",
    "    # 初始化用于评估的参数\n",
    "    frame_record = []\n",
    "    max_reward = -2000\n",
    "\n",
    "    # 实例化用于评估的智能体\n",
    "    Agent = RLAgent(action_size=eval_env.action_space.shape[0])\n",
    "\n",
    "    # 每个回合\n",
    "    for episode in tqdm(range(num_eval_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = eval_env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        current_action = None\n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "            \n",
    "        # 回合中的每一步\n",
    "        for step in range(max_steps):\n",
    "            # 处理当前状态\n",
    "            current_states = Agent.processing_states(frame_buffer)\n",
    "            # 选择动作\n",
    "            if step % frame_stack == 0:\n",
    "                output = Agent.select_action(current_states)\n",
    "                current_action = output[\"action\"].item()\n",
    "            # 执行该动作\n",
    "            observation, reward, terminated, truncated, info = eval_env.step(np.array([current_action]))\n",
    "            total_reward += reward\n",
    "            # 更新帧缓冲区\n",
    "            frame_buffer.append(observation)\n",
    "            # 如果需要记录视频, 则渲染画面 eval_env.render(), 然后将此画面添加到 frame_record 中\n",
    "            if need_record:\n",
    "                frame_record.append(eval_env.render())\n",
    "            # 判断是否结束\n",
    "            if terminated or truncated:\n",
    "                # 如果需要记录视频, 则保留最好的记录\n",
    "                if need_record and total_reward > max_reward:\n",
    "                    np_frame_record = np.array(frame_record)\n",
    "                    max_reward = total_reward\n",
    "                    frame_record.clear()\n",
    "                # 评估奖励\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                break\n",
    "\n",
    "    # 记录评估结果(只记录最好的奖励轮次)\n",
    "    if need_record:\n",
    "        record_file = f\"{os.path.abspath(os.path.join(save_dir, evaluate_record_perfix))}{int(max_reward)}.mp4\"\n",
    "        imageio.mimsave(record_file, np_frame_record, fps=evaluate_record_fps, quality=evaluate_record_quality)\n",
    "        logger.info(f\"The best evaluation record is: {record_file}\")\n",
    "\n",
    "    # 关闭环境\n",
    "    eval_env.close()\n",
    "    pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
