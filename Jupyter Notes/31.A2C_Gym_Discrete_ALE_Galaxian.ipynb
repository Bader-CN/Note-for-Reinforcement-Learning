{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 初始化 Jupyter 环境 & 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于在 Jupyter 中强制刷新参数\n",
    "%reset -f\n",
    "\n",
    "# 导入相关的包\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import ale_py\n",
    "import pygame\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions import Categorical\n",
    "from torchvision.transforms import v2\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 设置相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关功能\n",
    "is_training = 1                     # 是否进行训练\n",
    "is_evaluate = 1                     # 是否进行评估, 此时会渲染游戏画面\n",
    "need_record = 1                     # 是否开启录像, 前提是 is_evaluate=1 才有效, 不会渲染游戏画面\n",
    "\n",
    "# 日志等级\n",
    "log_level = \"INFO\"\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=log_level)\n",
    "\n",
    "# 环境信息\n",
    "env_id = \"ALE/Galaxian-v5\"          # 游戏环境名\n",
    "env_height = 210                    # 游戏画面高度\n",
    "env_width = 160                     # 游戏画面宽度\n",
    "max_steps = 10000                   # 每个回合的最大步数\n",
    "render_mode = \"rgb_array\"           # 渲染模式，可选 \"human\"、\"rgb_array\" 等\n",
    "\n",
    "# A2C 算法参数\n",
    "gamma = 0.99                        # 折扣因子\n",
    "memory_buffer_size = 10000          # 记忆缓存区大小\n",
    "frame_stack = 2                     # 帧堆叠的数量\n",
    "\n",
    "# 训练参数\n",
    "num_train_episodes = 10000          # 训练的总回合数\n",
    "lr = 1e-5                           # 学习率\n",
    "max_same_action = 16                # 最大连续相同动作次数，防止模型陷入局部最优解\n",
    "timestep_reward = 300               # 如果每隔指定的时间步, 并且生命值不减少的话, 则给予奖励\n",
    "\n",
    "# 评估参数\n",
    "num_eval_episodes = 10              # 评估的回合数\n",
    "reward_threshold = 2000             # 评估奖励阈值, 如果高于阈值时, 日志等级为 Success, 否则为 Warning\n",
    "eval_sample_action = True           # 评估时的动作是否基于概率来采样, True 则基于概率来选取动作, False 则直接选取最大概率\n",
    "\n",
    "\n",
    "# 保存策略\n",
    "save_dir = \"./Gym_ALE_Galaxian_A2C\"                     # 数据保存的目录\n",
    "save_freq = 100                                         # 模型保存的频率\n",
    "max_checkpoints = 5                                     # 最大保存的模型数量\n",
    "checkpoint_perfix_A = \"CheckPoint_Gym_ALE_Galaxian_A_\"  # 模型保存的前缀 Actor\n",
    "checkpoint_perfix_C = \"CheckPoint_Gym_ALE_Galaxian_C_\"  # 模型保存的前缀 Critic\n",
    "evaluate_record_perfix = \"Video_Gym_ALE_Galaxian_\"      # 评估记录保存的前缀\n",
    "evaluate_record_fps = 30                                # 评估记录保存的帧率\n",
    "evaluate_record_quality = 10                            # 评估记录保存的质量, 值为 0 ~ 10\n",
    "\n",
    "# 其余参数初始化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gym.register_envs(ale_py)                               # Arcade Learning Environment(ALE) 环境需要提前注册"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 预处理函数 & 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_checkpoint_id(checkpoint_perfix, save_dir=save_dir):\n",
    "    \"\"\"\n",
    "    获取最新的模型路径, 并返回 \"模型路径\" 和 checkpoint 对应的 id\n",
    "    \"\"\"\n",
    "    # 如果指定目录不存在, 则直接创建该目录\n",
    "    if not Path(save_dir).exists():\n",
    "        Path(save_dir).mkdir(parents=True)\n",
    "        logger.debug(\"The specified directory does not exist, will create this folder\")\n",
    "        return None\n",
    "    \n",
    "    # 获取所有的模型文件\n",
    "    checkpoints = []\n",
    "    current_path = Path(save_dir)\n",
    "    for entry in current_path.iterdir():\n",
    "        if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "            id = entry.name.split(checkpoint_perfix)[-1].split(\".\")[0]\n",
    "            checkpoints.append(id)\n",
    "    \n",
    "    # 寻找最大的 checkpoint id\n",
    "    if checkpoints.__len__() == 0:\n",
    "        logger.info(f\"Not found any {checkpoint_perfix} files, will random initialization of network parameters\")\n",
    "        return None\n",
    "    else:\n",
    "        max_checkpoint_id = max(checkpoints)\n",
    "        max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{max_checkpoint_id}.pth\")\n",
    "        logger.info(f\"Found max checkpoints, max_checkpoint_id is {max_checkpoint_id}\")\n",
    "        return {\"max_checkpoint_path\" : max_checkpoint_path, \"max_checkpoint_id\" : max_checkpoint_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_old_checkpoint(checkpoint_perfix, save_dir=save_dir, max_checkpoints=max_checkpoints):\n",
    "    \"\"\"\n",
    "    删除旧的模型文件, 只保留最新的 max_checkpoints 个模型文件\n",
    "    \"\"\"\n",
    "    if Path(save_dir).exists():\n",
    "        checkpoints = []\n",
    "        for entry in Path(save_dir).iterdir():\n",
    "            if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "                id = int(entry.name.split(checkpoint_perfix)[-1].split(\".\")[0])\n",
    "                checkpoints.append(id)\n",
    "    \n",
    "    if checkpoints.__len__() > max_checkpoints:\n",
    "        min_checkpoint_id = min(checkpoints)\n",
    "        min_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{min_checkpoint_id}.pth\")\n",
    "        os.remove(min_checkpoint_path)\n",
    "        logger.warning(f\"Delete old checkpoint file {min_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 定义智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    \"\"\"\n",
    "    智能体类, 封装了智能体所需要的各种方法\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size):\n",
    "        # Global Args\n",
    "        self.max_checkpoint_a = get_max_checkpoint_id(checkpoint_perfix=checkpoint_perfix_A)\n",
    "        self.max_checkpoint_c = get_max_checkpoint_id(checkpoint_perfix=checkpoint_perfix_C)\n",
    "        self.memory_buffer = deque(maxlen=memory_buffer_size)\n",
    "\n",
    "        # Init Actor Network\n",
    "        self.actor_network = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=frame_stack, out_channels=frame_stack * 2, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(in_channels=frame_stack * 2, out_channels=frame_stack * 4, kernel_size=3, stride=2, padding=1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.LazyLinear(out_features=1024),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=action_size),\n",
    "        )\n",
    "        if self.max_checkpoint_a is not None:\n",
    "            self.actor_network.load_state_dict(torch.load(self.max_checkpoint_a[\"max_checkpoint_path\"]))\n",
    "        \n",
    "        # Init Critic Network\n",
    "        self.critic_network = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=frame_stack, out_channels=frame_stack * 2, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(in_channels=frame_stack * 2, out_channels=frame_stack * 4, kernel_size=3, stride=2, padding=1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.LazyLinear(out_features=1024),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=1),\n",
    "        )\n",
    "        if self.max_checkpoint_c is not None:\n",
    "            self.critic_network.load_state_dict(torch.load(self.max_checkpoint_c[\"max_checkpoint_path\"]))\n",
    "\n",
    "\n",
    "        # Move to designated device\n",
    "        self.actor_network.to(device)\n",
    "        self.critic_network.to(device)\n",
    "\n",
    "        # Transfoms\n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Grayscale(1),\n",
    "        ])\n",
    "\n",
    "        # optimizer\n",
    "        self.a_optimizer = torch.optim.AdamW(self.actor_network.parameters(), lr=lr)\n",
    "        self.c_optimizer = torch.optim.AdamW(self.critic_network.parameters(), lr=lr)\n",
    "\n",
    "    def processing_states(self, frame_buffer):\n",
    "        \"\"\"\n",
    "        对输入的 frame_buffer 进行预处理, 并返回模型可以处理的 Tensor 对象\n",
    "        \"\"\"\n",
    "        # 将形状处理为 [batch_size=1, color_channel * stack_size, height, width]\n",
    "        states = torch.stack(tuple(self.transform(frame_buffer)), dim=0)\n",
    "        states = states.reshape(1, frame_stack, env_height, env_width)\n",
    "        logger.debug(f\"Processing states shape: {states.shape}\")\n",
    "        return states\n",
    "    \n",
    "    def select_action(self, state, sample=True):\n",
    "        \"\"\"\n",
    "        选择动作, 某些算法需要对模型的输出进行采样, 因此可以将 sample 设置为 True\n",
    "        :param state:  神经网络可以接收的输入形状: [batch_size, color_channel * stack_size, height, width]\n",
    "        :param sample: 动作是否是采样, 如果不是则直接选择概率最高的动作\n",
    "        \"\"\"\n",
    "        state = state.to(device)\n",
    "        if sample:\n",
    "            # https://pytorch.ac.cn/docs/stable/distributions.html#categorical\n",
    "            # 采样 & 动作的对数概率最好采用这种方法, 可以避免梯度消失的问题\n",
    "            logits = self.actor_network(state)\n",
    "            action_dist = Categorical(logits=logits)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            return {\"action\": action, \"log_prob\": log_prob}\n",
    "        else:\n",
    "            action_logits = self.actor_network(state)\n",
    "            action = action_logits.argmax(dim=1).item()\n",
    "            return {\"action\": action}\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        更新 A2C 算法的网络参数\n",
    "        \"\"\"\n",
    "        num_mems = len(self.memory_buffer)\n",
    "        logger.debug(f\"memory buffer size: {num_mems}\")\n",
    "\n",
    "        # 提取对应的数据\n",
    "        # 注意, 这里要提前处理形状, 防止在计算时广播导致形状不对\n",
    "        state = torch.cat([data[\"St\"] for data in self.memory_buffer], dim=0).to(device)\n",
    "        action = torch.tensor([data[\"At\"] for data in self.memory_buffer]).to(device)\n",
    "        reward = torch.tensor([data[\"Rt\"] for data in self.memory_buffer]).unsqueeze(1).to(device)\n",
    "        next_state = torch.cat([data[\"St+1\"] for data in self.memory_buffer], dim=0).to(device)\n",
    "        done = torch.tensor([data[\"Done\"] for data in self.memory_buffer]).float().unsqueeze(1).to(device)\n",
    "        logger.debug(f\"state shape: {state.shape}, action shape: {action.shape}, reward shape: {reward.shape}, next_state shape: {next_state.shape}, done shape: {done.shape}\")\n",
    "\n",
    "        # Critic 网络: TD目标 & TD误差\n",
    "        td_tgt = reward + gamma * self.critic_network(next_state) * (1 - done)\n",
    "        td_err = td_tgt - self.critic_network(state)\n",
    "\n",
    "        # Actor 网络：动作的概率分布\n",
    "        # 这里不使用 torch.log(self.actor_network(state).gather(1, action)) 来获取概率的对数\n",
    "        # 而是使用 Categorical 分布来获取 log_probs\n",
    "        logits = self.actor_network(state)\n",
    "        action_dist = Categorical(logits=logits)\n",
    "        log_probs = action_dist.log_prob(action)\n",
    "\n",
    "        # Actor 和 Critic 的损失函数\n",
    "        actor_loss = torch.mean(-log_probs * td_err.detach())\n",
    "        critic_loss = torch.mean(torch.nn.functional.mse_loss(self.critic_network(state), td_tgt.detach()))\n",
    "        logger.info(f\"actor_loss: {actor_loss:4f}, critic_loss: {critic_loss:4f}\")\n",
    "\n",
    "        self.a_optimizer.zero_grad()\n",
    "        self.c_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.a_optimizer.step()\n",
    "        self.c_optimizer.step()\n",
    "\n",
    "        # 清空经验池中的数据\n",
    "        self.memory_buffer.clear()\n",
    "    \n",
    "    def save_model(self, episodes):\n",
    "        \"\"\"\n",
    "        保存模型到指定路径, 并根据实际情况删除老的模型\n",
    "        \"\"\"\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint_a is None:\n",
    "            max_checkpoint_path_a = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_A}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path_a = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_A}{episodes + int(self.max_checkpoint_a[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint_c is None:\n",
    "            max_checkpoint_path_c = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_C}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path_c = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_C}{episodes + int(self.max_checkpoint_c[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        # 保存模型参数\n",
    "        torch.save(self.actor_network.state_dict(), max_checkpoint_path_a)\n",
    "        torch.save(self.critic_network.state_dict(), max_checkpoint_path_c)\n",
    "        logger.info(f\"Actor Model saved to {max_checkpoint_path_a}\")\n",
    "        logger.info(f\"Critic Model saved to {max_checkpoint_path_c}\")\n",
    "\n",
    "        # 删掉老模型\n",
    "        del_old_checkpoint(checkpoint_perfix=checkpoint_perfix_A)\n",
    "        del_old_checkpoint(checkpoint_perfix=checkpoint_perfix_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 调整环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定制环境\n",
    "class AleCustomEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    ALE 定制环境, 继承自 gym.Wrapper 类\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_lives = 4          # 剩余生命值\n",
    "        self.live_time = 0              # 生存时间, 超过一定时间会给予奖励\n",
    "        self.previous_action = None     # 上一次执行的动作\n",
    "        self.same_action_count = 0      # 重复动作的次数\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境, 这里定制了一些需要重置的参数\n",
    "        \"\"\"\n",
    "        # 重置观察结果\n",
    "        observation = self.env.reset()\n",
    "\n",
    "        # 重置相关计数\n",
    "        self.current_lives = 4\n",
    "        self.live_time = 0\n",
    "        self.previous_action = None\n",
    "        self.same_action_count = 0\n",
    "        self.same_action_display = False\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作, 并调整了env 的行为或奖励机制\n",
    "        \"\"\"\n",
    "        # 调用原始环境的 step 方法\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # 如果生命值发生变化, 则给予惩罚\n",
    "        if info['lives'] != self.current_lives:\n",
    "            self.current_lives = info['lives']\n",
    "            self.live_time = 0\n",
    "            reward = -100\n",
    "            logger.debug(f\"lives -1, current lives: {self.current_lives}\")\n",
    "        \n",
    "        # 鼓励存活时间\n",
    "        elif reward == 0:\n",
    "            self.live_time += 1\n",
    "            if self.live_time == timestep_reward:\n",
    "                self.live_time = 0\n",
    "                reward = 100\n",
    "                logger.debug(f\"live_time +100\")\n",
    "\n",
    "\n",
    "        # 如果重复次数过多, 则给予惩罚\n",
    "        if self.previous_action == action:\n",
    "            self.same_action_count += 1\n",
    "            if self.same_action_count >= max_same_action:\n",
    "                reward = -25\n",
    "                if self.same_action_display is False:\n",
    "                    self.same_action_display = True\n",
    "                    logger.error(f\"same action too many times, same_action_count = {self.same_action_count}\")\n",
    "        else:\n",
    "            same_action = self.same_action_count\n",
    "            self.same_action_count = 0\n",
    "            self.previous_action = action\n",
    "            if self.same_action_display is True:\n",
    "                self.same_action_display = False\n",
    "                logger.error(f\"same action it's over, total {same_action}\")\n",
    "\n",
    "        # 返回最终结果: observation, reward, terminated, truncated, info\n",
    "        return observation, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 训练智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    # 训练用的主环境\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = AleCustomEnv(env)\n",
    "\n",
    "    # 实例化智能体 (动作空间必须是离散的)\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_size = env.action_space.n\n",
    "        Agent = RLAgent(action_size=action_size)\n",
    "    else:\n",
    "        logger.error(\"Action space is not Discrete!\")\n",
    "        raise ValueError(\"Action space is not Discrete!\")\n",
    "\n",
    "    # 循环每个回合\n",
    "    for episode in tqdm(range(num_train_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        current_action = None\n",
    "        current_states = None\n",
    "        next_states = None\n",
    "        \n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "\n",
    "        # 回合中的每一步\n",
    "        for step in range(max_steps):\n",
    "            # 处理当前状态\n",
    "            current_states = Agent.processing_states(frame_buffer)\n",
    "\n",
    "            # 选择动作 & 对数概率\n",
    "            if step % frame_stack == 0:\n",
    "                output = Agent.select_action(current_states)\n",
    "                action, log_prob = output['action'].item(), output['log_prob']\n",
    "                current_action = action\n",
    "                logger.debug(f\"Selected action: {action}\")\n",
    "            # 执行当前动作: current_action & 更新帧缓冲区\n",
    "            observation, reward, terminated, truncated, info = env.step(current_action)\n",
    "            total_reward += reward\n",
    "            frame_buffer.append(observation)\n",
    "            next_states = Agent.processing_states(frame_buffer)\n",
    "            logger.debug(f\"Step {step + 1} | Reward: {reward} | Total Reward: {total_reward} | Terminated: {terminated} | Truncated: {truncated} | Info: {info}\")\n",
    "\n",
    "            # 保存到记忆区: 如果该帧是决策帧, 则新建记忆区记录\n",
    "            if step % frame_stack == 0:\n",
    "                Agent.memory_buffer.append({\"St\": current_states, \"At\": current_action, \"Rt\": reward, \"St+1\": next_states, \"Done\": terminated})\n",
    "            # 如果该帧不是决策帧, 则调整 & 完善记忆区记录\n",
    "            else:\n",
    "                # 奖励叠加\n",
    "                Agent.memory_buffer[-1][\"Rt\"] += reward\n",
    "                # 将 St+1 替换为最新的状态\n",
    "                Agent.memory_buffer[-1][\"St+1\"] = next_states\n",
    "\n",
    "            # 判断是否结束该回合\n",
    "            if terminated or truncated:\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Episode finish, total step {step + 1} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Episode finish, total step {step + 1} | Total Reward: {total_reward}\")\n",
    "                total_reward = 0\n",
    "                Agent.memory_buffer[-1][\"Done\"] = terminated\n",
    "                break\n",
    "        \n",
    "        # 更新模型\n",
    "        Agent.update()\n",
    "        \n",
    "        # 保存模型\n",
    "        if (episode + 1) % save_freq == 0 and episode != 0:\n",
    "            episodes = episode + 1\n",
    "            Agent.save_model(episodes)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 评估智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估但不录制视频\n",
    "if is_evaluate == 1 and need_record == 0:\n",
    "    eval_env = gym.make(env_id, render_mode=\"human\")\n",
    "    eval_env = AleCustomEnv(eval_env)\n",
    "# 评估且需要录制视频\n",
    "elif is_evaluate == 1 and need_record == 1:\n",
    "    eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    eval_env = AleCustomEnv(eval_env)\n",
    "\n",
    "# 如果启用了评估\n",
    "if is_evaluate == 1:\n",
    "    # 初始化用于评估的参数\n",
    "    frame_record = []\n",
    "    max_reward = 0\n",
    "\n",
    "    # 实例化用于评估的智能体\n",
    "    Agent = RLAgent(action_size=eval_env.action_space.n)\n",
    "\n",
    "    # 每个回合\n",
    "    for episode in tqdm(range(num_eval_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = eval_env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        current_action = None\n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "            \n",
    "        # 回合中的每一步\n",
    "        for step in range(max_steps):\n",
    "            # 处理当前状态\n",
    "            current_states = Agent.processing_states(frame_buffer)\n",
    "            # 选择动作\n",
    "            if step % frame_stack == 0:\n",
    "                output = Agent.select_action(current_states, sample=eval_sample_action)\n",
    "                current_action = output[\"action\"].item()\n",
    "            # 执行该动作\n",
    "            observation, reward, terminated, truncated, info = eval_env.step(current_action)\n",
    "            total_reward += reward\n",
    "            # 更新帧缓冲区\n",
    "            frame_buffer.append(observation)\n",
    "            # 如果需要记录视频, 则渲染画面 eval_env.render(), 然后将此画面添加到 frame_record 中\n",
    "            if need_record:\n",
    "                frame_record.append(eval_env.render())\n",
    "            # 判断是否结束\n",
    "            if terminated or truncated:\n",
    "                # 如果需要记录视频, 则保留最好的记录\n",
    "                if need_record and total_reward > max_reward:\n",
    "                    np_frame_record = np.array(frame_record)\n",
    "                    max_reward = total_reward\n",
    "                    frame_record.clear()\n",
    "                # 评估奖励\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                break\n",
    "\n",
    "    # 记录评估结果(只记录最好的奖励轮次)\n",
    "    if need_record:\n",
    "        record_file = f\"{os.path.abspath(os.path.join(save_dir, evaluate_record_perfix))}{int(max_reward)}.mp4\"\n",
    "        imageio.mimsave(record_file, np_frame_record, fps=evaluate_record_fps, quality=evaluate_record_quality)\n",
    "        logger.info(f\"The best evaluation record is: {record_file}\")\n",
    "\n",
    "    # 关闭环境\n",
    "    eval_env.close()\n",
    "    pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
