{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 初始化 Jupyter 环境 & 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于在 Jupyter 中强制刷新参数\n",
    "%reset -f\n",
    "\n",
    "# 导入相关的包\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pygame\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions import Categorical\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 设置相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关功能\n",
    "is_training = 1                     # 是否进行训练\n",
    "is_evaluate = 0                     # 是否进行评估, 此时会渲染游戏画面\n",
    "need_record = 0                     # 是否开启录像, 前提是 is_evaluate=1 才有效, 不会渲染游戏画面\n",
    "\n",
    "# 日志等级\n",
    "log_level = \"INFO\"\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=log_level)\n",
    "\n",
    "# 环境信息\n",
    "env_id = \"CartPole-v1\"              # 游戏环境名\n",
    "max_steps = 500                     # 每个回合的最大步数\n",
    "render_mode = \"rgb_array\"           # 渲染模式，可选 \"human\"、\"rgb_array\" 等\n",
    "\n",
    "# TRPO 算法参数\n",
    "memory_buffer_size = 10000          # 记忆缓存区大小\n",
    "frame_stack = 2                     # 帧堆叠的数量\n",
    "gamma = 0.95                        # 折扣因子, 控制未来奖励的重要性\n",
    "lmbda = 0.9                         # GAE 参数, 控制轨迹长度\n",
    "kl_constraint = 0.001               # KL散度距离约束\n",
    "linesearch_alpha = 0.9              # 线性搜索的 alpha\n",
    "epslion = 1e-8                      # 用于数值稳定性的小数\n",
    "\n",
    "# 训练参数\n",
    "num_train_episodes = 2000           # 训练的总回合数\n",
    "lr = 2e-3                           # 学习率\n",
    "\n",
    "# 评估参数\n",
    "num_eval_episodes = 10              # 评估的回合数\n",
    "reward_threshold = 500              # 评估奖励阈值, 如果高于阈值时, 日志等级为 Success, 否则为 Warning\n",
    "eval_sample_action = True           # 评估时的动作是否基于概率来采样, True 则基于概率来选取动作, False 则直接选取最大概率\n",
    "\n",
    "\n",
    "# 保存策略\n",
    "save_dir = \"./Gym_Classic_CartPole_TRPO_FrameStack\"              # 数据保存的目录\n",
    "save_freq = 100                                                  # 模型保存的频率\n",
    "max_checkpoints = 5                                              # 最大保存的模型数量\n",
    "checkpoint_perfix_A = \"CheckPoint_Gym_Classic_CartPole_A_\"       # 模型保存的前缀 Actor\n",
    "checkpoint_perfix_C = \"CheckPoint_Gym_Classic_CartPole_C_\"       # 模型保存的前缀 Critic\n",
    "evaluate_record_perfix = \"Video_Gym_Classic_CartPole_\"           # 评估记录保存的前缀\n",
    "evaluate_record_fps = 30                                         # 评估记录保存的帧率\n",
    "evaluate_record_quality = 10                                     # 评估记录保存的质量, 值为 0 ~ 10\n",
    "\n",
    "# 其余参数初始化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.autograd.set_detect_anomaly(True)                        # 开启 PyTorch 自动微分异常检测, 用于调试 NaN 梯度问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 预处理函数 & 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_checkpoint_id(checkpoint_perfix, save_dir=save_dir):\n",
    "    \"\"\"\n",
    "    获取最新的模型路径, 并返回 \"模型路径\" 和 checkpoint 对应的 id\n",
    "    \"\"\"\n",
    "    # 如果指定目录不存在, 则直接创建该目录\n",
    "    if not Path(save_dir).exists():\n",
    "        Path(save_dir).mkdir(parents=True)\n",
    "        logger.debug(\"The specified directory does not exist, will create this folder\")\n",
    "        return None\n",
    "    \n",
    "    # 获取所有的模型文件\n",
    "    checkpoints = []\n",
    "    current_path = Path(save_dir)\n",
    "    for entry in current_path.iterdir():\n",
    "        if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "            id = entry.name.split(checkpoint_perfix)[-1].split(\".\")[0]\n",
    "            checkpoints.append(int(id))\n",
    "    \n",
    "    # 寻找最大的 checkpoint id\n",
    "    if checkpoints.__len__() == 0:\n",
    "        logger.info(f\"Not found any {checkpoint_perfix} files, will random initialization of network parameters\")\n",
    "        return None\n",
    "    else:\n",
    "        max_checkpoint_id = max(checkpoints)\n",
    "        max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{max_checkpoint_id}.pth\")\n",
    "        logger.info(f\"Found max checkpoints, max_checkpoint_id is {max_checkpoint_id}\")\n",
    "        return {\"max_checkpoint_path\" : max_checkpoint_path, \"max_checkpoint_id\" : max_checkpoint_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_old_checkpoint(checkpoint_perfix, save_dir=save_dir, max_checkpoints=max_checkpoints):\n",
    "    \"\"\"\n",
    "    删除旧的模型文件, 只保留最新的 max_checkpoints 个模型文件\n",
    "    \"\"\"\n",
    "    if Path(save_dir).exists():\n",
    "        checkpoints = []\n",
    "        for entry in Path(save_dir).iterdir():\n",
    "            if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "                id = int(entry.name.split(checkpoint_perfix)[-1].split(\".\")[0])\n",
    "                checkpoints.append(id)\n",
    "    \n",
    "    if checkpoints.__len__() > max_checkpoints:\n",
    "        min_checkpoint_id = min(checkpoints)\n",
    "        min_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{min_checkpoint_id}.pth\")\n",
    "        os.remove(min_checkpoint_path)\n",
    "        logger.warning(f\"Delete old checkpoint file {min_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 定义智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    \"\"\"\n",
    "    智能体类, 封装了智能体所需要的各种方法\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size):\n",
    "        # Global Args\n",
    "        self.max_checkpoint_a = get_max_checkpoint_id(checkpoint_perfix=checkpoint_perfix_A)\n",
    "        self.max_checkpoint_c = get_max_checkpoint_id(checkpoint_perfix=checkpoint_perfix_C)\n",
    "        self.memory_buffer = deque(maxlen=memory_buffer_size)\n",
    "\n",
    "        # Init Actor Network\n",
    "        self.actor_network = torch.nn.Sequential(\n",
    "            torch.nn.LazyLinear(out_features=128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=action_size),\n",
    "        )\n",
    "        if self.max_checkpoint_a is not None:\n",
    "            self.actor_network.load_state_dict(torch.load(self.max_checkpoint_a[\"max_checkpoint_path\"]))\n",
    "        \n",
    "        # Init Critic Network\n",
    "        self.critic_network = torch.nn.Sequential(\n",
    "            torch.nn.LazyLinear(out_features=128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=1),\n",
    "        )\n",
    "        if self.max_checkpoint_c is not None:\n",
    "            self.critic_network.load_state_dict(torch.load(self.max_checkpoint_c[\"max_checkpoint_path\"]))\n",
    "\n",
    "        # Move to designated device\n",
    "        self.actor_network.to(device)\n",
    "        self.critic_network.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        self.c_optimizer = torch.optim.AdamW(self.critic_network.parameters(), lr=lr)\n",
    "\n",
    "    def processing_states(self, frame_buffer):\n",
    "        \"\"\"\n",
    "        对输入的 frame_buffer 进行预处理, 并返回模型可以处理的 Tensor 对象\n",
    "        \"\"\"\n",
    "        # 将形状处理为 [1, size * frame_buffer_size]\n",
    "        states = torch.tensor(np.array(frame_buffer))\n",
    "        states = states.reshape(1, -1)\n",
    "        logger.debug(f\"Processing states shape: {states.shape}\")\n",
    "        return states\n",
    "    \n",
    "    def select_action(self, state, sample=True):\n",
    "        \"\"\"\n",
    "        选择动作, Policy Gradient 算法需要对模型的输出进行采样\n",
    "        :param state:  神经网络可以接收的输入形状: [batch_size, color_channel * stack_size, height, width]\n",
    "        :param sample: 动作是否是采样, 如果不是则直接选择概率最高的动作\n",
    "        \"\"\"\n",
    "        state = state.to(device)\n",
    "        if sample:\n",
    "            # https://pytorch.ac.cn/docs/stable/distributions.html#categorical\n",
    "            # 采样 & 动作的对数概率最好采用这种方法, 可以避免梯度消失的问题\n",
    "            logits = self.actor_network(state)\n",
    "            action_dist = Categorical(logits=logits)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action).detach()\n",
    "            return {\"action\": action, \"log_prob\": log_prob}\n",
    "        else:\n",
    "            action_logits = self.actor_network(state)\n",
    "            action = action_logits.argmax(dim=1).item()\n",
    "            return {\"action\": action}\n",
    "    \n",
    "    def hessian_matrix_vector_product(self, state, old_action_dists, vector):\n",
    "        \"\"\"\n",
    "        计算黑塞矩阵和一个向量的乘积, 即 (Hessian-Vector Product, HVP)\n",
    "        该方法用于 TRPO 中的 \"共轭梯度法(conjugate gradient)\", 用来求解步长方向\n",
    "        \"\"\"\n",
    "        # 获取新的 action 状态分布\n",
    "        new_action_dists = torch.distributions.Categorical(logits=self.actor_network(state))\n",
    "        # 计算新旧策略分布之间的平均 \"KL散度\"\n",
    "        kl = torch.mean(torch.distributions.kl.kl_divergence(old_action_dists, new_action_dists))\n",
    "        # 对 \"KL散度\" 关于 \"策略参数\" 求一阶导数(梯度), 并保留计算图 (create_graph=True), 以便之后还能继续对这个梯度再求导\n",
    "        kl_grad = torch.autograd.grad(kl, self.actor_network.parameters(), create_graph=True, retain_graph=True)\n",
    "        # 把梯度参数展平, 拼成一个向量\n",
    "        kl_grad_vector = torch.cat([grad.view(-1) for grad in kl_grad])\n",
    "        # 计算 \"KL散度\" 的梯度 和 外部给定的向量的点积 (这一步是自动微分技巧的关键)\n",
    "        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)\n",
    "        # 对上一步结果再次对 \"策略参数\" 求导, 这就是 Hessian-Vector Product (HVP)\n",
    "        grad2 = torch.autograd.grad(kl_grad_vector_product, self.actor_network.parameters(), retain_graph=True)\n",
    "        # 展平并拼成一个向量并返回\n",
    "        grad2_vector = torch.cat([grad.view(-1) for grad in grad2])\n",
    "        return grad2_vector\n",
    "    \n",
    "    def conjugate_gradient(self, grad, state, old_action_dists):\n",
    "        \"\"\"\n",
    "        共轭梯度法求解方程\n",
    "        \"\"\"\n",
    "        x = torch.zeros_like(grad)\n",
    "        r = grad.clone()\n",
    "        p = grad.clone()\n",
    "        rdotr = torch.dot(r, r)\n",
    "\n",
    "        # 共轭梯度主循环\n",
    "        for i in range(40):\n",
    "            Hp = self.hessian_matrix_vector_product(state, old_action_dists, p)\n",
    "            # 数值稳定性: alpha = rdotr / torch.dot(p, Hp)\n",
    "            # torch.sign(denom) 会返回 denom 的符号, 通过这种方式可以保留原始输入的符号\n",
    "                # 如果为正数则返回 1.0; \n",
    "                # 如果为负数则返回 -1.0; \n",
    "                # 如果为零则返回 0.0\n",
    "            # 乘以 epslion 就是得到一个很小但带有原始符号的数\n",
    "            denom = torch.dot(p, Hp)\n",
    "            denom = denom if torch.abs(denom) > epslion else torch.sign(denom) * epslion\n",
    "            alpha = rdotr / denom\n",
    "\n",
    "            x += alpha * p\n",
    "            r -= alpha * Hp\n",
    "            new_rdotr = torch.dot(r, r)\n",
    "            if new_rdotr < epslion:\n",
    "                logger.debug(f\"conjugate gradient loop number: {i+1}\")\n",
    "                break\n",
    "            beta = new_rdotr / rdotr\n",
    "            p = r + beta * p\n",
    "            rdotr = new_rdotr\n",
    "        return x\n",
    "    \n",
    "    def compute_surrogate_obj(self, state, actions, advantage, old_log_probs, actor_network): \n",
    "        \"\"\"\n",
    "        用于评估在指定策略网络参数下, 策略的期望提升程度, 即评估策略目标函数\n",
    "        注意: actor_network 是必须要传入的参数, 因此评估的就是指定 actor_network 参数\n",
    "        \"\"\"\n",
    "        log_probs = torch.distributions.Categorical(logits=actor_network(state)).log_prob(actions)\n",
    "        # 如果新旧概率分布相同, 则 ratio 的结果为 1\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        return torch.mean(ratio * advantage)\n",
    "\n",
    "    def line_search(self, state, actions, advantage, old_log_probs, old_action_dists, max_vec):\n",
    "        \"\"\"\n",
    "        线性搜索\n",
    "        \"\"\"\n",
    "        old_para = torch.nn.utils.convert_parameters.parameters_to_vector(self.actor_network.parameters())\n",
    "        # 计算在指定 \"策略网络参数\" 的情况下, 目标函数的期望估计(old_obj)\n",
    "        old_obj = self.compute_surrogate_obj(state, actions, advantage, old_log_probs, self.actor_network)\n",
    "        # 先复制, 然后再需要时直接转换参数\n",
    "        new_actor = copy.deepcopy(self.actor_network).to(device=device)\n",
    "\n",
    "        # 线性搜索主循环\n",
    "        for i in range(15):\n",
    "            # 每次缩小的比例\n",
    "            coef = linesearch_alpha**i\n",
    "\n",
    "            # 求出 actor_network 的新参数, 并赋值给 new_actor\n",
    "            new_para = old_para + coef * max_vec\n",
    "            torch.nn.utils.convert_parameters.vector_to_parameters(new_para, new_actor.parameters())\n",
    "\n",
    "            try:\n",
    "                # 利用 new_actor 求概率分布, \"KL散度\", 目标函数的期望估计(new_obj)\n",
    "                new_action_dists = torch.distributions.Categorical(logits=new_actor(state))\n",
    "                kl_div = torch.mean(torch.distributions.kl.kl_divergence(old_action_dists, new_action_dists))\n",
    "                new_obj = self.compute_surrogate_obj(state, actions, advantage, old_log_probs, new_actor)\n",
    "                # 比较新老参数优劣\n",
    "                if new_obj > old_obj and kl_div < kl_constraint:\n",
    "                    logger.info(\"line search result: updated actor_network parameters\")\n",
    "                    logger.debug(f\"new_obj: {new_obj:.6f} | old_obj: {old_obj:.6f} | kl_div: {kl_div:.6f}\")\n",
    "                    return new_para\n",
    "            except Exception as e:\n",
    "                logger.error(\"line search result: numerical anomaly, skip this update\")\n",
    "                return old_para\n",
    "        logger.warning(\"line search result: use old actor_network parameters\")\n",
    "        logger.warning(f\"new_obj: {new_obj:.6f} | old_obj: {old_obj:.6f} | kl_div: {kl_div:.6f}\")\n",
    "        return old_para\n",
    "    \n",
    "    def policy_learn(self, state, actions, old_action_dists, old_log_probs, advantage):\n",
    "        \"\"\"\n",
    "        更新 actor 网络参数\n",
    "        \"\"\"\n",
    "        # 计算在指定 \"策略网络参数\" 的情况下, 目标函数的期望估计(old_obj)\n",
    "        surrogate_obj = self.compute_surrogate_obj(state, actions, advantage, old_log_probs, self.actor_network)\n",
    "        # 求 surrogate_obj = f(actor_network(state)) 这个函数的梯度\n",
    "        # f(x) 代表着根据当前策略分布、旧策略分布、优势函数等计算出来的目标函数, 也就是 compute_surrogate_obj\n",
    "        grads = torch.autograd.grad(surrogate_obj, self.actor_network.parameters(), retain_graph=True)\n",
    "        # 展平梯度, 以备后续计算\n",
    "        obj_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n",
    "\n",
    "        # 检查相关变量的值是否合法\n",
    "        if all((grad is not None) and torch.all(grad == 0) for grad in grads):\n",
    "            # grads 的类型是 tuple, 因此需要分别判断\n",
    "            logger.error(f\"grad's value is NaN or all 0!\")     \n",
    "        if torch.all(obj_grad == 0):\n",
    "            logger.error(f\"obj_grad's value is be all 0!\")\n",
    "\n",
    "        # 用共轭梯度法计算x = H^(-1)g\n",
    "        descent_direction = self.conjugate_gradient(obj_grad, state, old_action_dists)\n",
    "        # Hessian 向量积\n",
    "        Hd = self.hessian_matrix_vector_product(state, old_action_dists, descent_direction)\n",
    "        # max_coef 就是需要更新的步长\n",
    "        max_coef = torch.sqrt(2 * kl_constraint / (torch.dot(descent_direction, Hd) + epslion))\n",
    "\n",
    "        # 检查相关变量的值是否合法\n",
    "        if torch.isnan(descent_direction).any():\n",
    "            logger.error(f\"descent_direction contains NaN!\")\n",
    "        if torch.isnan(Hd).any():\n",
    "            logger.error(f\"Hd contains NaN!\")\n",
    "        if torch.isnan(max_coef):\n",
    "            logger.error(f\"max_coef is NaN!\")\n",
    "\n",
    "        # 线性搜索\n",
    "        new_para = self.line_search(state, actions, advantage, old_log_probs, old_action_dists, descent_direction * max_coef)\n",
    "        # 用线性搜索后的参数更新策略\n",
    "        torch.nn.utils.convert_parameters.vector_to_parameters(new_para, self.actor_network.parameters())\n",
    "\n",
    "    def compute_advantage(self, gamma, lmbda, td_err):\n",
    "        \"\"\"\n",
    "        广义优势估计 (Generalized Advantage Estimation, GAE)\n",
    "        Args:\n",
    "        - gamma:  折扣因子 (0 ~ 1), 控制未来奖励的重要性\n",
    "        - lmbda:  GAE 衰减参数 (0 ~ 1), 控制轨迹长度\n",
    "        - td_err: 时间差分误差 (TD Error) 的张量\n",
    "        \"\"\"\n",
    "        # 将 td_err 从计算图中卸载下来, 避免影响梯度传递(核心思想为对于常量, 最好都执行 detach 来避免影响梯度传递)\n",
    "        td_err = td_err.detach().numpy()\n",
    "        advantage_list = []\n",
    "        advantage = 0.0\n",
    "        # 逆序遍历 td_err\n",
    "        for delta in td_err[::-1]:\n",
    "            # 核心公式: A_t = γλA_{t+1} + δ_t\n",
    "            # gamma * lmbda 控制信息衰减的乘子 (γλ)\n",
    "            # delta 当前时间步的 td_err (δ_t)\n",
    "            advantage = gamma * lmbda * advantage + delta\n",
    "            advantage_list.append(advantage)\n",
    "        # 将计算结果逆序回原始顺序 (因为之前是逆序的)\n",
    "        advantage_list.reverse()\n",
    "        # 将列表转换为 np.array, 这样转换为 tensor 的速度会更快\n",
    "        advantages = np.array(advantage_list, dtype=np.float32)\n",
    "        return torch.from_numpy(advantages)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        TRPO 算法更新\n",
    "        \"\"\"\n",
    "        num_mems = len(self.memory_buffer)\n",
    "        logger.debug(f\"memory buffer size: {num_mems}\")\n",
    "\n",
    "        # 提取对应的数据\n",
    "        # 注意, 这里要提前处理形状, 防止在计算时广播导致形状不对\n",
    "        state = torch.cat([data[\"St\"] for data in self.memory_buffer], dim=0).to(device)\n",
    "        action = torch.tensor([data[\"At\"] for data in self.memory_buffer]).to(device)\n",
    "        reward = torch.tensor([data[\"Rt\"] for data in self.memory_buffer]).unsqueeze(1).to(device)\n",
    "        next_state = torch.cat([data[\"St+1\"] for data in self.memory_buffer], dim=0).to(device)\n",
    "        done = torch.tensor([data[\"Done\"] for data in self.memory_buffer]).float().unsqueeze(1).to(device)\n",
    "        logger.debug(f\"state shape: {state.shape}, action shape: {action.shape}, reward shape: {reward.shape}, next_state shape: {next_state.shape}, done shape: {done.shape}\")\n",
    "\n",
    "        # Critic 网络: TD目标 & TD误差\n",
    "        td_tgt = reward + gamma * self.critic_network(next_state) * (1 - done)\n",
    "        td_err = td_tgt - self.critic_network(state)\n",
    "\n",
    "        advantage = self.compute_advantage(gamma, lmbda, td_err.cpu()).to(device)\n",
    "        # logger.info(f\"advantage std: {advantage.std()} | advantage mean: {advantage.mean()}\")\n",
    "\n",
    "        # detach() 方法一定要添加, 目的是将 logits 从计算图中移除, 即不保存梯度, 让它变成常量\n",
    "        # https://github.com/boyu-ai/Hands-on-RL/issues/96\n",
    "        old_action_dists = torch.distributions.Categorical(logits=self.actor_network(state).detach())\n",
    "        # 这里要求的是对应 action 的概率, 而不是对应 state 下所有 action 的概率分布\n",
    "        # detach() 方法一定要添加, 目的是将 old_log_probs 从计算图中移除, 即不保存梯度, 让它变成常量\n",
    "        # https://github.com/boyu-ai/Hands-on-RL/issues/96\n",
    "        old_log_probs = old_action_dists.log_prob(action).detach()\n",
    "        # 检查并核对 old_log_probs 中是否包含 nan 值\n",
    "        if torch.isnan(old_log_probs).any():\n",
    "            logger.error(f\"old_log_probs size: {old_log_probs.shape}\")\n",
    "            logger.error(f\"old_log_probs contains nan: {torch.isnan(old_log_probs).any()}\")\n",
    "        \n",
    "        # 更新价值函数\n",
    "        critic_loss = torch.mean(torch.nn.functional.mse_loss(self.critic_network(state), td_tgt.detach()))\n",
    "        # logger.success(f\"critic_loss: {critic_loss.item():.4f}\")\n",
    "        self.c_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.c_optimizer.step()\n",
    "\n",
    "        # 更新策略函数\n",
    "        self.policy_learn(state, action, old_action_dists, old_log_probs, advantage)\n",
    "\n",
    "        # 清空经验池中的数据\n",
    "        self.memory_buffer.clear()\n",
    "    \n",
    "    def save_model(self, episodes):\n",
    "        \"\"\"\n",
    "        保存模型到指定路径, 并根据实际情况删除老的模型\n",
    "        \"\"\"\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint_a is None:\n",
    "            max_checkpoint_path_a = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_A}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path_a = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_A}{episodes + int(self.max_checkpoint_a[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint_c is None:\n",
    "            max_checkpoint_path_c = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_C}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path_c = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_C}{episodes + int(self.max_checkpoint_c[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        # 保存模型参数\n",
    "        torch.save(self.actor_network.state_dict(), max_checkpoint_path_a)\n",
    "        torch.save(self.critic_network.state_dict(), max_checkpoint_path_c)\n",
    "        logger.info(f\"Actor Model saved to {max_checkpoint_path_a}\")\n",
    "        logger.info(f\"Critic Model saved to {max_checkpoint_path_c}\")\n",
    "\n",
    "        # 删掉老模型\n",
    "        del_old_checkpoint(checkpoint_perfix=checkpoint_perfix_A)\n",
    "        del_old_checkpoint(checkpoint_perfix=checkpoint_perfix_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 调整环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定制环境\n",
    "class CustomEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    定制环境, 继承自 gym.Wrapper 类, 用于修改环境的行为或奖励机制\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境\n",
    "        \"\"\"\n",
    "        # 重置观察结果\n",
    "        observation = self.env.reset()\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作, 并调整了env 的行为或奖励机制\n",
    "        \"\"\"\n",
    "        # 调用原始环境的 step 方法\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # 返回最终结果: observation, reward, terminated, truncated, info\n",
    "        return observation, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 训练智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    # 训练用的主环境\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = CustomEnv(env)\n",
    "\n",
    "    # 实例化智能体 (动作空间必须是离散的)\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_size = env.action_space.n\n",
    "        Agent = RLAgent(action_size=action_size)\n",
    "    else:\n",
    "        logger.error(\"Action space is not Discrete!\")\n",
    "        raise ValueError(\"Action space is not Discrete!\")\n",
    "\n",
    "    # 循环每个回合\n",
    "    for episode in tqdm(range(num_train_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        current_action = None\n",
    "        current_states = None\n",
    "        next_states = None\n",
    "        \n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "\n",
    "        # 回合中的每一步\n",
    "        for step in range(max_steps):\n",
    "            # 处理当前状态\n",
    "            current_states = Agent.processing_states(frame_buffer)\n",
    "\n",
    "            # 选择动作 & 对数概率\n",
    "            if step % frame_stack == 0:\n",
    "                output = Agent.select_action(current_states)\n",
    "                action, log_prob = output['action'].item(), output['log_prob']\n",
    "                current_action = action\n",
    "                logger.debug(f\"Selected action: {action}\")\n",
    "            # 执行当前动作: current_action & 更新帧缓冲区\n",
    "            observation, reward, terminated, truncated, info = env.step(current_action)\n",
    "            total_reward += reward\n",
    "            frame_buffer.append(observation)\n",
    "            next_states = Agent.processing_states(frame_buffer)\n",
    "            logger.debug(f\"Step {step + 1} | Reward: {reward} | Total Reward: {total_reward} | Terminated: {terminated} | Truncated: {truncated} | Info: {info}\")\n",
    "\n",
    "            # 保存到记忆区: 如果该帧是决策帧, 则新建记忆区记录\n",
    "            if step % frame_stack == 0:\n",
    "                Agent.memory_buffer.append({\"St\": current_states, \"At\": current_action, \"Rt\": reward, \"St+1\": next_states, \"Done\": terminated})\n",
    "            # 如果该帧不是决策帧, 则调整 & 完善记忆区记录\n",
    "            else:\n",
    "                # 奖励叠加\n",
    "                Agent.memory_buffer[-1][\"Rt\"] += reward\n",
    "                # 将 St+1 替换为最新的状态\n",
    "                Agent.memory_buffer[-1][\"St+1\"] = next_states\n",
    "\n",
    "            # 判断是否结束该回合\n",
    "            if terminated or truncated:\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Episode finish, total step {step + 1} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Episode finish, total step {step + 1} | Total Reward: {total_reward}\")\n",
    "                total_reward = 0\n",
    "                Agent.memory_buffer[-1][\"Done\"] = terminated\n",
    "                break\n",
    "        \n",
    "        # 更新模型\n",
    "        Agent.update()\n",
    "        \n",
    "        # 保存模型\n",
    "        if (episode + 1) % save_freq == 0 and episode != 0:\n",
    "            episodes = episode + 1\n",
    "            Agent.save_model(episodes)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 评估智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估但不录制视频\n",
    "if is_evaluate == 1 and need_record == 0:\n",
    "    eval_env = gym.make(env_id, render_mode=\"human\")\n",
    "    eval_env = CustomEnv(eval_env)\n",
    "# 评估且需要录制视频\n",
    "elif is_evaluate == 1 and need_record == 1:\n",
    "    eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    eval_env = CustomEnv(eval_env)\n",
    "\n",
    "# 如果启用了评估\n",
    "if is_evaluate == 1:\n",
    "    # 初始化用于评估的参数\n",
    "    frame_record = []\n",
    "    max_reward = 0\n",
    "\n",
    "    # 实例化用于评估的智能体\n",
    "    Agent = RLAgent(action_size=eval_env.action_space.n)\n",
    "\n",
    "    # 每个回合\n",
    "    for episode in tqdm(range(num_eval_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = eval_env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        current_action = None\n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "            \n",
    "        # 回合中的每一步\n",
    "        for step in range(max_steps):\n",
    "            # 处理当前状态\n",
    "            current_states = Agent.processing_states(frame_buffer)\n",
    "            # 选择动作\n",
    "            if step % frame_stack == 0:\n",
    "                output = Agent.select_action(current_states, sample=eval_sample_action)\n",
    "                current_action = output[\"action\"].item()\n",
    "            # 执行该动作\n",
    "            observation, reward, terminated, truncated, info = eval_env.step(current_action)\n",
    "            total_reward += reward\n",
    "            # 更新帧缓冲区\n",
    "            frame_buffer.append(observation)\n",
    "            # 如果需要记录视频, 则渲染画面 eval_env.render(), 然后将此画面添加到 frame_record 中\n",
    "            if need_record:\n",
    "                frame_record.append(eval_env.render())\n",
    "            # 判断是否结束\n",
    "            if terminated or truncated:\n",
    "                # 如果需要记录视频, 则保留最好的记录\n",
    "                if need_record and total_reward > max_reward:\n",
    "                    np_frame_record = np.array(frame_record)\n",
    "                    max_reward = total_reward\n",
    "                    frame_record.clear()\n",
    "                # 评估奖励\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                break\n",
    "\n",
    "    # 记录评估结果(只记录最好的奖励轮次)\n",
    "    if need_record:\n",
    "        record_file = f\"{os.path.abspath(os.path.join(save_dir, evaluate_record_perfix))}{int(max_reward)}.mp4\"\n",
    "        imageio.mimsave(record_file, np_frame_record, fps=evaluate_record_fps, quality=evaluate_record_quality)\n",
    "        logger.info(f\"The best evaluation record is: {record_file}\")\n",
    "\n",
    "    # 关闭环境\n",
    "    eval_env.close()\n",
    "    pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
