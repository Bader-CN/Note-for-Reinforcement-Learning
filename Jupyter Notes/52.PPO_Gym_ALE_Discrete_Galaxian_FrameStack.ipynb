{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d488f673",
   "metadata": {},
   "source": [
    "## Step1 初始化 Jupyter 环境 & 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于在 Jupyter 中强制刷新参数\n",
    "%reset -f\n",
    "\n",
    "# 导入相关的包\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import ale_py\n",
    "import pygame\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms import v2\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443b3b6",
   "metadata": {},
   "source": [
    "## Step2 设置相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a09f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关功能\n",
    "is_training = 1                     # 是否进行训练\n",
    "is_evaluate = 0                     # 是否进行评估, 此时会渲染游戏画面\n",
    "need_record = 0                     # 是否开启录像, 前提是 is_evaluate=1 才有效, 不会渲染游戏画面\n",
    "\n",
    "# 日志等级\n",
    "log_level = \"INFO\"\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=log_level)\n",
    "\n",
    "# 环境信息\n",
    "env_id = \"ALE/Galaxian-v5\"          # 游戏环境名\n",
    "env_height = 128                    # 游戏画面高度\n",
    "env_width = 128                     # 游戏画面宽度\n",
    "max_steps = 10000                   # 每个回合的最大步数\n",
    "render_mode = \"rgb_array\"           # 渲染模式，可选 \"human\"、\"rgb_array\" 等\n",
    "\n",
    "# PPO 算法参数\n",
    "frame_stack = 4                     # 帧堆叠的数量\n",
    "gamma = 0.98                        # 折扣因子, 控制未来奖励的重要性\n",
    "lmbda = 0.95                        # GAE 参数, 控制轨迹长度\n",
    "clip_eps = 0.2                      # PPO 截断的范围\n",
    "epoch = 10                          # 样本重复训练的次数\n",
    "\n",
    "# 训练参数\n",
    "num_train_episodes = 30000          # 训练的总回合数\n",
    "a_lr = 5e-6                         # actor 学习率\n",
    "c_lr = 1e-5                         # critic 学习率\n",
    "max_same_action = 60                # 最大连续相同动作次数，防止模型陷入局部最优解\n",
    "timestep_reward = 300               # 如果每隔指定的时间步, 并且生命值不减少的话, 则给予奖励/惩罚\n",
    "\n",
    "# 评估参数\n",
    "num_eval_episodes = 10              # 评估的回合数\n",
    "reward_threshold = 1000             # 评估奖励阈值, 如果高于阈值时, 日志等级为 Success, 否则为 Warning\n",
    "is_sample_action = False           # 是否采样动作进行评估, 如果为 False 则使用模型最优动作进行评估\n",
    "eval_of_reward = 50                 # 保存用于评估 Reward 的数值, 数值为最多仅统计最近多少个\n",
    "eval_of_step = 50                   # 保存用于评估 Step 的数值, 数值为最多仅统计最近多少个\n",
    "\n",
    "# 保存策略\n",
    "save_dir = \"./Gym_ALE_Galaxian_PPO_FrameStack\"                   # 数据保存的目录\n",
    "save_freq = 100                                                  # 模型保存的频率\n",
    "max_checkpoints = 5                                              # 最大保存的模型数量\n",
    "checkpoint_perfix_A = \"CheckPoint_Gym_ALE_Galaxian_A_\"           # 模型保存的前缀 Actor\n",
    "checkpoint_perfix_C = \"CheckPoint_Gym_ALE_Galaxian_C_\"           # 模型保存的前缀 Critic\n",
    "evaluate_record_perfix = \"Video_Gym_ALE_Galaxian_\"               # 评估记录保存的前缀\n",
    "evaluate_record_fps = 30                                         # 评估记录保存的帧率\n",
    "evaluate_record_quality = 10                                     # 评估记录保存的质量, 值为 0 ~ 10\n",
    "\n",
    "# 其余参数初始化\n",
    "# device = \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gym.register_envs(ale_py)            # Arcade Learning Environment(ALE) 环境需要提前注册"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbb191",
   "metadata": {},
   "source": [
    "## Step3 预处理函数 & 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d012ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_checkpoint_id(checkpoint_perfix, save_dir=save_dir):\n",
    "    \"\"\"\n",
    "    获取最新的模型路径, 并返回 \"模型路径\" 和 checkpoint 对应的 id\n",
    "    \"\"\"\n",
    "    # 如果指定目录不存在, 则直接创建该目录\n",
    "    if not Path(save_dir).exists():\n",
    "        Path(save_dir).mkdir(parents=True)\n",
    "        logger.debug(\"The specified directory does not exist, will create this folder\")\n",
    "        return None\n",
    "    \n",
    "    # 获取所有的模型文件\n",
    "    checkpoints = []\n",
    "    current_path = Path(save_dir)\n",
    "    for entry in current_path.iterdir():\n",
    "        if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "            id = entry.name.split(checkpoint_perfix)[-1].split(\".\")[0]\n",
    "            checkpoints.append(int(id))\n",
    "    \n",
    "    # 寻找最大的 checkpoint id\n",
    "    if checkpoints.__len__() == 0:\n",
    "        logger.info(f\"Not found any {checkpoint_perfix} files, will random initialization of network parameters\")\n",
    "        return None\n",
    "    else:\n",
    "        max_checkpoint_id = max(checkpoints)\n",
    "        max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{max_checkpoint_id}.pth\")\n",
    "        logger.info(f\"Found max checkpoints, max_checkpoint_id is {max_checkpoint_id}\")\n",
    "        return {\"max_checkpoint_path\" : max_checkpoint_path, \"max_checkpoint_id\" : max_checkpoint_id}\n",
    "\n",
    "def del_old_checkpoint(checkpoint_perfix, save_dir=save_dir, max_checkpoints=max_checkpoints):\n",
    "    \"\"\"\n",
    "    删除旧的模型文件, 只保留最新的 max_checkpoints 个模型文件\n",
    "    \"\"\"\n",
    "    if Path(save_dir).exists():\n",
    "        checkpoints = []\n",
    "        for entry in Path(save_dir).iterdir():\n",
    "            if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "                id = int(entry.name.split(checkpoint_perfix)[-1].split(\".\")[0])\n",
    "                checkpoints.append(id)\n",
    "    \n",
    "    if checkpoints.__len__() > max_checkpoints:\n",
    "        min_checkpoint_id = min(checkpoints)\n",
    "        min_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{min_checkpoint_id}.pth\")\n",
    "        os.remove(min_checkpoint_path)\n",
    "        logger.warning(f\"Delete old checkpoint file {min_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94910549",
   "metadata": {},
   "source": [
    "## Step4 定义智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b401cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    策略网络\n",
    "    \"\"\"\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=frame_stack, out_channels=frame_stack * 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=frame_stack * 2, out_channels=frame_stack * 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.LazyLinear(out_features=256)\n",
    "        self.fc2 = torch.nn.LazyLinear(out_features=256)\n",
    "        self.fc3 = torch.nn.LazyLinear(out_features=action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.mish(self.fc1(x))\n",
    "        x = F.mish(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CriticNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    价值网络\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=frame_stack, out_channels=frame_stack * 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=frame_stack * 2, out_channels=frame_stack * 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.LazyLinear(out_features=256)\n",
    "        self.fc2 = torch.nn.LazyLinear(out_features=256)\n",
    "        self.fc3 = torch.nn.LazyLinear(out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.mish(self.fc1(x))\n",
    "        x = F.mish(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c84b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    智能体代理类, 封装了各种方法\n",
    "    \"\"\"\n",
    "    def __init__(self, action_dim):\n",
    "        # Global Args\n",
    "        self.max_checkpoint_a = get_max_checkpoint_id(checkpoint_perfix=checkpoint_perfix_A)\n",
    "        self.max_checkpoint_c = get_max_checkpoint_id(checkpoint_perfix=checkpoint_perfix_C)\n",
    "        self.memory = {\"St\":[], \"At\":[], \"Rt\":[], \"St+1\":[], \"Done\":[]}\n",
    "\n",
    "        # Evaluation Args\n",
    "        self.loss_a = []\n",
    "        self.loss_c = []\n",
    "        self.reward = deque(maxlen=eval_of_reward)\n",
    "        self.step = deque(maxlen=eval_of_step)\n",
    "\n",
    "        # Init Actor Network\n",
    "        self.a_net = ActorNet(action_dim)\n",
    "        self.c_net = CriticNet()\n",
    "        if self.max_checkpoint_a is not None:\n",
    "            self.a_net.load_state_dict(torch.load(self.max_checkpoint_a[\"max_checkpoint_path\"]))\n",
    "        if self.max_checkpoint_c is not None:\n",
    "            self.c_net.load_state_dict(torch.load(self.max_checkpoint_c[\"max_checkpoint_path\"]))\n",
    "        self.a_net.to(device)\n",
    "        self.c_net.to(device)\n",
    "\n",
    "        # Init Optimizer\n",
    "        self.a_optimizer = torch.optim.AdamW(self.a_net.parameters(), lr=a_lr)\n",
    "        self.c_optimizer = torch.optim.AdamW(self.c_net.parameters(), lr=c_lr)\n",
    "\n",
    "        # Transfoms\n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Grayscale(1),\n",
    "            v2.Resize((128, 128)),\n",
    "        ])\n",
    "\n",
    "    def processing_states(self, frame_buffer):\n",
    "        \"\"\"\n",
    "        对输入的 frame_buffer 进行预处理, 并返回模型可以处理的 Tensor 对象\n",
    "        \"\"\"\n",
    "        # 将形状处理为 [batch_size=1, color_channel * stack_size, height, width]\n",
    "        states = torch.stack(tuple(self.transform(frame_buffer)), dim=0)\n",
    "        states = states.reshape(1, frame_stack, env_height, env_width)\n",
    "        logger.debug(f\"Processing states shape: {states.shape}\")\n",
    "        return states\n",
    "    \n",
    "    def select_action(self, state, sample=True):\n",
    "        \"\"\"\n",
    "        选择动作, 某些算法需要对模型的输出进行采样, 因此可以将 sample 设置为 True\n",
    "        :param state:  神经网络可以接收的输入形状: [batch_size, color_channel * stack_size, height, width]\n",
    "        :param sample: 动作是否是采样, 如果不是则直接选择概率最高的动作\n",
    "        \"\"\"\n",
    "        state = state.to(device)\n",
    "        if sample:\n",
    "            # https://pytorch.ac.cn/docs/stable/distributions.html#categorical\n",
    "            # 采样 & 动作的对数概率最好采用这种方法, 可以避免梯度消失的问题\n",
    "            logits = self.a_net(state)\n",
    "            action_dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = action_dist.sample()\n",
    "            return action.item()\n",
    "        else:\n",
    "            action_logits = self.a_net(state)\n",
    "            action = action_logits.argmax(dim=1)\n",
    "            return action.item()\n",
    "    \n",
    "    def compute_advantage(self, gamma, lmbda, td_err):\n",
    "        \"\"\"\n",
    "        广义优势估计 (Generalized Advantage Estimation, GAE)\n",
    "        Args:\n",
    "        - gamma:  折扣因子 (0 ~ 1), 控制未来奖励的重要性\n",
    "        - lmbda:  GAE 衰减参数 (0 ~ 1), 控制轨迹长度\n",
    "        - td_err: 时间差分误差 (TD Error) 的张量\n",
    "        \"\"\"\n",
    "        # 将 td_err 从计算图中卸载下来, 避免影响梯度传递(核心思想为对于常量, 最好都执行 detach 来避免影响梯度传递)\n",
    "        td_err = td_err.cpu().detach().numpy()\n",
    "        advantage_list = []\n",
    "        advantage = 0.0\n",
    "        # 逆序遍历 td_err\n",
    "        for delta in td_err[::-1]:\n",
    "            # 核心公式: A_t = γλA_{t+1} + δ_t\n",
    "            # gamma * lmbda 控制信息衰减的乘子 (γλ)\n",
    "            # delta 当前时间步的 td_err (δ_t)\n",
    "            advantage = gamma * lmbda * advantage + delta\n",
    "            advantage_list.append(advantage)\n",
    "        # 将计算结果逆序回原始顺序 (因为之前是逆序的)\n",
    "        advantage_list.reverse()\n",
    "        # 将列表转换为 np.array, 这样转换为 tensor 的速度会更快\n",
    "        advantages = np.array(advantage_list, dtype=np.float32)\n",
    "        return torch.from_numpy(advantages).reshape(-1, 1).to(device)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        PPO 算法更新\n",
    "        \"\"\"\n",
    "        # 提取对应的数据(数据也是常量, 因此不需要计算梯度)\n",
    "        # 注意, 这里要提前处理形状, 防止在计算时广播导致形状不对\n",
    "        with torch.no_grad():\n",
    "            states = torch.stack(self.memory[\"St\"]).float().squeeze(1).to(device)\n",
    "            actions = torch.tensor(self.memory[\"At\"], dtype=torch.long).unsqueeze(1).to(device)\n",
    "            rewards = torch.tensor(self.memory[\"Rt\"], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            next_states = torch.stack(self.memory[\"St+1\"]).float().squeeze(1).to(device)\n",
    "            dones = torch.tensor(self.memory[\"Done\"], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            logger.debug(f\"states shape: {states.shape}, actions shape: {actions.shape}, rewards shape: {rewards.shape}, next_states shape: {next_states.shape}, dones shape: {dones.shape}\")\n",
    "\n",
    "            # 对奖励进行归一化\n",
    "            # rewards = (rewards - rewards.mean()) / rewards.std()\n",
    "\n",
    "        # 这里不需要保留和计算梯度, 只作为常量来看待, 否则会影响梯度传播, 例如 old_log_probs 和 old_action_dists\n",
    "        # https://github.com/boyu-ai/Hands-on-RL/issues/96\n",
    "        # 要么在 with torch.no_grad() 下获取, 要么手动执行 detach() 方法主动分离计算图\n",
    "        with torch.no_grad():\n",
    "            # Critic 网络: TD目标 & TD误差\n",
    "            td_tgt = rewards + gamma * self.c_net(next_states) * (1 - dones)\n",
    "            td_err = td_tgt - self.c_net(states)\n",
    "            # GAE 的结果\n",
    "            advantage = self.compute_advantage(gamma, lmbda, td_err)\n",
    "            # 可选: 优势函数归一化, 提高稳定性\n",
    "            # advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "            # 动作概率\n",
    "            old_action_dists = torch.distributions.Categorical(logits=self.a_net(states))\n",
    "            old_log_probs = old_action_dists.log_prob(actions.squeeze())\n",
    "        \n",
    "        # 更新主循环\n",
    "        for _ in range(epoch):\n",
    "            # 获取当前的动作概率\n",
    "            new_action_dists = torch.distributions.Categorical(logits=self.a_net(states))\n",
    "            log_probs = new_action_dists.log_prob(actions.squeeze())\n",
    "            # 熵正则化\n",
    "            entropy = new_action_dists.entropy().mean() * 0.01\n",
    "            # 计算新老概率的差距\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-clip_eps, 1+clip_eps) * advantage\n",
    "            # 损失函数\n",
    "            a_loss = -torch.min(surr1, surr2).mean() - entropy\n",
    "            c_loss = F.mse_loss(self.c_net(states), td_tgt.detach(), reduction='mean')\n",
    "            # 执行优化\n",
    "            self.a_optimizer.zero_grad()\n",
    "            a_loss.backward()\n",
    "            self.loss_a.append(a_loss.item())\n",
    "            self.a_optimizer.step()\n",
    "            self.c_optimizer.zero_grad()\n",
    "            c_loss.backward()\n",
    "            self.loss_c.append(c_loss.item())\n",
    "            self.c_optimizer.step()\n",
    "\n",
    "        logger.info(f\"Mean Actor Loss: {np.mean(self.loss_a):.4f} | Mean Critic Loss: {np.mean(self.loss_c):.4f}\")\n",
    "        self.loss_a.clear()\n",
    "        self.loss_c.clear()\n",
    "        \n",
    "        # 清空经验池中的数据\n",
    "        self.memory[\"St\"].clear()\n",
    "        self.memory[\"At\"].clear()\n",
    "        self.memory[\"Rt\"].clear()\n",
    "        self.memory[\"St+1\"].clear()\n",
    "        self.memory[\"Done\"].clear()\n",
    "\n",
    "    def save_model(self, episodes):\n",
    "        \"\"\"\n",
    "        保存模型到指定路径, 并根据实际情况删除老的模型\n",
    "        \"\"\"\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint_a is None:\n",
    "            max_checkpoint_path_a = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_A}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path_a = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_A}{episodes + int(self.max_checkpoint_a[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint_c is None:\n",
    "            max_checkpoint_path_c = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_C}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path_c = os.path.abspath(f\"{save_dir}/{checkpoint_perfix_C}{episodes + int(self.max_checkpoint_c[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        # 保存模型参数\n",
    "        torch.save(self.a_net.state_dict(), max_checkpoint_path_a)\n",
    "        torch.save(self.c_net.state_dict(), max_checkpoint_path_c)\n",
    "        logger.info(f\"Actor Model saved to {max_checkpoint_path_a}\")\n",
    "        logger.info(f\"Critic Model saved to {max_checkpoint_path_c}\")\n",
    "\n",
    "        # 删掉老模型\n",
    "        del_old_checkpoint(checkpoint_perfix=checkpoint_perfix_A)\n",
    "        del_old_checkpoint(checkpoint_perfix=checkpoint_perfix_C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee1f9a",
   "metadata": {},
   "source": [
    "## Step5 调整环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定制环境\n",
    "class AleCustomEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    ALE 定制环境, 继承自 gym.Wrapper 类\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_lives = 4          # 剩余生命值\n",
    "        self.live_time = 0              # 生存时间, 超过一定时间会给予奖励\n",
    "        self.previous_action = None     # 上一次执行的动作\n",
    "        self.same_action_count = 0      # 重复动作的次数\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境, 这里定制了一些需要重置的参数\n",
    "        \"\"\"\n",
    "        # 重置观察结果\n",
    "        observation = self.env.reset()\n",
    "\n",
    "        # 重置相关计数\n",
    "        self.current_lives = 4\n",
    "        self.live_time = 0\n",
    "        self.previous_action = None\n",
    "        self.same_action_count = 0\n",
    "        self.same_action_display = False\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作, 并调整了env 的行为或奖励机制\n",
    "        \"\"\"\n",
    "        # 调用原始环境的 step 方法\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # 如果生命值发生变化, 则给予惩罚\n",
    "        if info['lives'] != self.current_lives:\n",
    "            self.current_lives = info['lives']\n",
    "            self.live_time = 0\n",
    "            reward = -100\n",
    "            logger.debug(f\"lives -1, current lives: {self.current_lives}\")\n",
    "        \n",
    "        # 鼓励/惩罚不作为的时间\n",
    "        elif reward == 0:\n",
    "            self.live_time += 1\n",
    "            if self.live_time == timestep_reward:\n",
    "                self.live_time = 0\n",
    "                reward = -10\n",
    "                logger.debug(f\"live_time -10\")\n",
    "\n",
    "        # 如果重复次数过多, 则给予惩罚\n",
    "        if self.previous_action == action:\n",
    "            self.same_action_count += 1\n",
    "            if self.same_action_count >= max_same_action:\n",
    "                reward = -50\n",
    "                if self.same_action_display is False:\n",
    "                    self.same_action_display = True\n",
    "                    logger.error(f\"same action too many times, same_action_count = {self.same_action_count}\")\n",
    "        else:\n",
    "            same_action = self.same_action_count\n",
    "            self.same_action_count = 0\n",
    "            self.previous_action = action\n",
    "            if self.same_action_display is True:\n",
    "                self.same_action_display = False\n",
    "                logger.error(f\"same action it's over, total {same_action}\")\n",
    "\n",
    "        # 返回最终结果: observation, reward, terminated, truncated, info\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349fa9c",
   "metadata": {},
   "source": [
    "## Step6 训练智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    # 训练用的主环境\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = AleCustomEnv(env)\n",
    "\n",
    "    # 实例化智能体 (动作空间必须是离散的)\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n\n",
    "        agent = Agent(action_dim=action_dim)\n",
    "    else:\n",
    "        logger.error(\"Action space is not Discrete!\")\n",
    "        raise ValueError(\"Action space is not Discrete!\")\n",
    "    \n",
    "    # 循环每个回合\n",
    "    for episode in tqdm(range(num_train_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "\n",
    "        # 回合中的每一步\n",
    "        while steps < max_steps:\n",
    "            # 1. 预处理当前状态\n",
    "            state = agent.processing_states(frame_buffer)\n",
    "            # 2. 选择动作, 每个宏观步骤只选择一次\n",
    "            action = agent.select_action(state)\n",
    "\n",
    "            # 初始化累积奖励和结束标志\n",
    "            accumulated_reward = 0\n",
    "            done = False\n",
    "            # 3. 在一个宏观步骤中, 重复执行相同动作 frame_stack 次\n",
    "            for _ in range(frame_stack):\n",
    "                observation, reward, terminated, truncated, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                accumulated_reward += reward\n",
    "                # 更新帧缓冲区 & 步数加一\n",
    "                frame_buffer.append(observation)\n",
    "                steps += 1\n",
    "                # 如果中途回合结束, 则立即跳出内部循环\n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "                    break\n",
    "            \n",
    "            # 4. 在宏观步骤结束后，处理下一个状态\n",
    "            next_state = agent.processing_states(frame_buffer)\n",
    "            # 5. 保存到经验区\n",
    "            agent.memory[\"St\"].append(state)\n",
    "            agent.memory[\"At\"].append(action)\n",
    "            agent.memory[\"Rt\"].append(accumulated_reward)\n",
    "            agent.memory[\"St+1\"].append(next_state)\n",
    "            agent.memory[\"Done\"].append(done)\n",
    "\n",
    "            # 6. 判断是否结束\n",
    "            if done:\n",
    "                # 整理统计数据\n",
    "                agent.reward.append(total_reward)\n",
    "                agent.step.append(steps)\n",
    "                # 判断日志显示的颜色\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Episode {episode + 1} | Total steps {steps} | Total Reward: {total_reward} | Mean Step: {np.mean(agent.step):.2f} | Mean Reward: {np.mean(agent.reward):.2f}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Episode {episode + 1} | Total steps {steps} | Total Reward: {total_reward} | Mean Step: {np.mean(agent.step):.2f} | Mean Reward: {np.mean(agent.reward):.2f}\")\n",
    "                break\n",
    "        \n",
    "        # 更新模型\n",
    "        agent.update()\n",
    "        \n",
    "        # 保存模型\n",
    "        if (episode + 1) % save_freq == 0 and episode != 0:\n",
    "            episodes = episode + 1\n",
    "            agent.save_model(episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13c63b",
   "metadata": {},
   "source": [
    "## Step7 评估智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db50b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估但不录制视频\n",
    "if is_evaluate == 1 and need_record == 0:\n",
    "    eval_env = gym.make(env_id, render_mode=\"human\")\n",
    "    eval_env = AleCustomEnv(eval_env)\n",
    "# 评估且需要录制视频\n",
    "elif is_evaluate == 1 and need_record == 1:\n",
    "    eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    eval_env = AleCustomEnv(eval_env)\n",
    "\n",
    "# 如果启用了评估\n",
    "if is_evaluate == 1:\n",
    "    # 初始化用于评估的参数\n",
    "    frame_record = []\n",
    "    max_reward = None\n",
    "\n",
    "    # 实例化用于评估的智能体\n",
    "    agent = Agent(action_dim=eval_env.action_space.n)\n",
    "    \n",
    "    # 循环每个回合\n",
    "    for episode in tqdm(range(num_eval_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = eval_env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "\n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "\n",
    "        # 回合中的每一步\n",
    "        while steps < max_steps:\n",
    "            # 1. 预处理当前状态\n",
    "            state = agent.processing_states(frame_buffer)\n",
    "            # 2. 选择动作, 每个宏观步骤只选择一次\n",
    "            action = agent.select_action(state, sample=is_sample_action)\n",
    "\n",
    "            # 初始化结束标志\n",
    "            done = False\n",
    "            # 3. 在一个宏观步骤中, 重复执行相同动作 frame_stack 次\n",
    "            for _ in range(frame_stack):\n",
    "                observation, reward, terminated, truncated, info = eval_env.step(action)\n",
    "                total_reward += reward\n",
    "                # 更新帧缓冲区 & 步数加一\n",
    "                frame_buffer.append(observation)\n",
    "                # 是否需要录像\n",
    "                if need_record:\n",
    "                    frame_record.append(eval_env.render())\n",
    "                steps += 1\n",
    "                # 如果中途回合结束, 则立即跳出内部循环\n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "                    break\n",
    "\n",
    "            # 4. 判断是否结束\n",
    "            if done:\n",
    "                # 整理统计数据\n",
    "                agent.reward.append(total_reward)\n",
    "                agent.step.append(steps)\n",
    "                # 判断日志显示的颜色\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Episode {episode + 1} | Total steps {steps} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Episode {episode + 1} | Total steps {steps} | Total Reward: {total_reward}\")\n",
    "                # 如果需要记录视频, 则保留最好的记录\n",
    "                if need_record and (max_reward is None or total_reward > max_reward):\n",
    "                    np_frame_record = np.array(frame_record)\n",
    "                    max_reward = total_reward\n",
    "                    frame_record.clear()\n",
    "                break\n",
    "\n",
    "    # 记录评估结果(只记录最好的奖励轮次)\n",
    "    if need_record:\n",
    "        record_file = f\"{os.path.abspath(os.path.join(save_dir, evaluate_record_perfix))}{int(max_reward)}.mp4\"\n",
    "        imageio.mimsave(record_file, np_frame_record, fps=evaluate_record_fps, quality=evaluate_record_quality)\n",
    "        logger.info(f\"The best evaluation record is: {record_file}\")\n",
    "\n",
    "    # 关闭环境\n",
    "    eval_env.close()\n",
    "    pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
