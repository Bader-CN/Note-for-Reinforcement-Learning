{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 初始化 Jupyter 环境 & 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于在 Jupyter 中强制刷新参数\n",
    "%reset -f\n",
    "\n",
    "# 导入相关的包\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pygame\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions import Categorical\n",
    "from torchvision.transforms import v2\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 设置相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相关功能\n",
    "is_training = 1                     # 是否进行训练\n",
    "is_evaluate = 1                     # 是否进行评估, 此时会渲染游戏画面\n",
    "need_record = 1                     # 是否开启录像, 前提是 is_evaluate=1 才有效, 不会渲染游戏画面\n",
    "\n",
    "# 日志等级\n",
    "log_level = \"INFO\"\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=log_level)\n",
    "\n",
    "# 环境信息\n",
    "env_id = \"CartPole-v1\"              # 游戏环境名\n",
    "max_steps = 500                     # 每个回合的最大步数\n",
    "render_mode = \"rgb_array\"           # 渲染模式，可选 \"human\"、\"rgb_array\" 等\n",
    "\n",
    "# Policy Gradient 算法参数\n",
    "gamma = 0.95                        # 折扣因子\n",
    "memory_buffer_size = 10000          # 记忆缓存区大小\n",
    "frame_stack = 2                     # 帧堆叠的数量\n",
    "\n",
    "# 训练参数\n",
    "num_train_episodes = 1000           # 训练的总回合数\n",
    "lr = 1e-3                           # 学习率\n",
    "\n",
    "# 评估参数\n",
    "num_eval_episodes = 10              # 评估的回合数\n",
    "reward_threshold = 500              # 评估奖励阈值, 如果高于阈值时, 日志等级为 Success, 否则为 Warning\n",
    "eval_sample_action = True           # 评估时的动作是否基于概率来采样, True 则基于概率来选取动作, False 则直接选取最大概率\n",
    "\n",
    "\n",
    "# 保存策略\n",
    "save_dir = \"./Gym_Classic_CartPole_Policy_Gradient_FrameStack\"   # 数据保存的目录\n",
    "save_freq = 100                                                  # 模型保存的频率\n",
    "max_checkpoints = 5                                              # 最大保存的模型数量\n",
    "checkpoint_perfix = \"CheckPoint_Gym_Classic_CartPole_FS_\"        # 模型保存的前缀\n",
    "evaluate_record_perfix = \"Video_Gym_Classic_CartPole_FS_\"        # 评估记录保存的前缀\n",
    "evaluate_record_fps = 30                                         # 评估记录保存的帧率\n",
    "evaluate_record_quality = 10                                     # 评估记录保存的质量, 值为 0 ~ 10\n",
    "\n",
    "# 其余参数初始化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 预处理函数 & 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_checkpoint_id(save_dir=save_dir, checkpoint_perfix=checkpoint_perfix):\n",
    "    \"\"\"\n",
    "    获取最新的模型路径, 并返回 \"模型路径\" 和 checkpoint 对应的 id\n",
    "    \"\"\"\n",
    "    # 如果指定目录不存在, 则直接创建该目录\n",
    "    if not Path(save_dir).exists():\n",
    "        Path(save_dir).mkdir(parents=True)\n",
    "        logger.debug(\"The specified directory does not exist, will create this folder\")\n",
    "        return None\n",
    "    \n",
    "    # 获取所有的模型文件\n",
    "    checkpoints = []\n",
    "    current_path = Path(save_dir)\n",
    "    for entry in current_path.iterdir():\n",
    "        if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "            id = entry.name.split(checkpoint_perfix)[-1].split(\".\")[0]\n",
    "            checkpoints.append(id)\n",
    "    \n",
    "    # 寻找最大的 checkpoint id\n",
    "    if checkpoints.__len__() == 0:\n",
    "        logger.info(\"Not found any checkpoint files, will random initialization of network parameters\")\n",
    "        return None\n",
    "    else:\n",
    "        max_checkpoint_id = max(checkpoints)\n",
    "        max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{max_checkpoint_id}.pth\")\n",
    "        logger.info(f\"Found max checkpoints, max_checkpoint_id is {max_checkpoint_id}\")\n",
    "        return {\"max_checkpoint_path\" : max_checkpoint_path, \"max_checkpoint_id\" : max_checkpoint_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_old_checkpoint(save_dir=save_dir, checkpoint_perfix=checkpoint_perfix, max_checkpoints=max_checkpoints):\n",
    "    \"\"\"\n",
    "    删除旧的模型文件, 只保留最新的 max_checkpoints 个模型文件\n",
    "    \"\"\"\n",
    "    if Path(save_dir).exists():\n",
    "        checkpoints = []\n",
    "        for entry in Path(save_dir).iterdir():\n",
    "            if entry.is_file() and entry.suffix == \".pth\" and entry.name.startswith(checkpoint_perfix):\n",
    "                id = int(entry.name.split(checkpoint_perfix)[-1].split(\".\")[0])\n",
    "                checkpoints.append(id)\n",
    "    \n",
    "    if checkpoints.__len__() > max_checkpoints:\n",
    "        min_checkpoint_id = min(checkpoints)\n",
    "        min_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{min_checkpoint_id}.pth\")\n",
    "        os.remove(min_checkpoint_path)\n",
    "        logger.warning(f\"Delete old checkpoint file {min_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 定义智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    \"\"\"\n",
    "    智能体类, 封装了智能体所需要的各种方法\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size):\n",
    "        # Global Args\n",
    "        self.max_checkpoint = get_max_checkpoint_id()\n",
    "        self.memory_buffer = deque(maxlen=memory_buffer_size)\n",
    "\n",
    "        # Init Network\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.LazyLinear(out_features=128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.LazyLinear(out_features=action_size),\n",
    "        )\n",
    "        if self.max_checkpoint is not None:\n",
    "            self.network.load_state_dict(torch.load(self.max_checkpoint[\"max_checkpoint_path\"]))\n",
    "\n",
    "        # Move to designated device\n",
    "        self.network.to(device)\n",
    "\n",
    "        # Transfoms\n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Grayscale(1),\n",
    "        ])\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.AdamW(self.network.parameters(), lr=lr)\n",
    "\n",
    "    def processing_states(self, frame_buffer):\n",
    "        \"\"\"\n",
    "        对输入的 frame_buffer 进行预处理, 并返回模型可以处理的 Tensor 对象\n",
    "        \"\"\"\n",
    "        # 将形状处理为 [1, size * frame_buffer_size]\n",
    "        states = torch.stack(tuple(self.transform(frame_buffer)), dim=0)\n",
    "        states = states.reshape(1, -1)\n",
    "        logger.debug(f\"Processing states shape: {states.shape}\")\n",
    "        return states\n",
    "    \n",
    "    def select_action(self, state, sample=True):\n",
    "        \"\"\"\n",
    "        选择动作, 某些算法需要对模型的输出进行采样, 因此可以将 sample 设置为 True\n",
    "        :param state:  神经网络可以接收的输入形状: [batch_size, color_channel * stack_size, height, width]\n",
    "        :param sample: 动作是否是采样, 如果不是则直接选择概率最高的动作\n",
    "        \"\"\"\n",
    "        state = state.to(device)\n",
    "        if sample:\n",
    "            # https://pytorch.ac.cn/docs/stable/distributions.html#categorical\n",
    "            # 采样 & 动作的对数概率最好采用这种方法, 可以避免梯度消失的问题\n",
    "            logits = self.network(state)\n",
    "            action_dist = Categorical(logits=logits)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            return {\"action\": action, \"log_prob\": log_prob}\n",
    "        else:\n",
    "            action_logits = self.network(state)\n",
    "            action = action_logits.argmax(dim=1).item()\n",
    "            return {\"action\": action}\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"\n",
    "        更新 policy 参数\n",
    "        \"\"\"\n",
    "        G_t = 0\n",
    "        G_t_deque = deque()\n",
    "        num_mems = len(self.memory_buffer)\n",
    "        # 计算每一步的回报 G_t\n",
    "        for i in range(num_mems)[::-1]:\n",
    "            G_t = G_t * gamma + self.memory_buffer[i][\"Rt\"]\n",
    "            G_t_deque.appendleft(G_t)\n",
    "        logger.debug(f\"G_t values: {G_t_deque}\")\n",
    "        # 对 G_t 进行标准化\n",
    "        G_stand = torch.tensor(G_t_deque)\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        G_stand = ((G_stand - G_stand.mean()) / (G_stand.std() + eps)).to(device)\n",
    "        # 计算梯度误差\n",
    "        # 这里的期望是让 log_prob * G_stand 最大化, 即回报的奖励值最大化\n",
    "        # 但由于梯度更新的反向传播(backward) 会让 log_prob * G_stand 最小化, 因此这里取负号 (-log_prob * G_stand), 这样效果是等价的\n",
    "        policy_loss = []\n",
    "        for i in range(num_mems):\n",
    "            log_prob = self.memory_buffer[i][\"log_prob\"]\n",
    "            policy_loss.append(-log_prob * G_stand[i])\n",
    "        # 更新梯度信息(使用了梯度累积, 将所有样本的梯度信息累加起来, 然后再进行反向传播和参数更新)\n",
    "        # 实测这里也可以写为 torch.cat(policy_loss).sum().to(device)\n",
    "        policy_loss = torch.stack(policy_loss).sum().to(device)\n",
    "        logger.info(f\"Policy_Loss: {policy_loss.item():4f}\")\n",
    "        # 更新策略网络参数\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # 清空经验池中的数据\n",
    "        self.memory_buffer.clear()\n",
    "    \n",
    "    def save_model(self, episodes):\n",
    "        \"\"\"\n",
    "        保存模型到指定路径, 并根据实际情况删除老的模型\n",
    "        \"\"\"\n",
    "        # 没有任何已存在的模型文件, 即首次启动训练\n",
    "        if self.max_checkpoint is None:\n",
    "            max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{episodes}.pth\")\n",
    "        # 已存在模型文件的情况\n",
    "        else:\n",
    "            max_checkpoint_path = os.path.abspath(f\"{save_dir}/{checkpoint_perfix}{episodes + int(self.max_checkpoint[\"max_checkpoint_id\"])}.pth\")\n",
    "\n",
    "        torch.save(self.network.state_dict(), max_checkpoint_path)\n",
    "        logger.info(f\"Model saved to {max_checkpoint_path}\")\n",
    "        # 删掉老模型\n",
    "        del_old_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 调整环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定制环境\n",
    "class CustomEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    定制环境, 继承自 gym.Wrapper 类, 用于修改环境的行为或奖励机制\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境\n",
    "        \"\"\"\n",
    "        # 重置观察结果\n",
    "        observation = self.env.reset()\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作, 并调整了env 的行为或奖励机制\n",
    "        \"\"\"\n",
    "        # 调用原始环境的 step 方法\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # 返回最终结果: observation, reward, terminated, truncated, info\n",
    "        return observation, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 训练智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    # 训练用的主环境\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = CustomEnv(env)\n",
    "\n",
    "    # 实例化智能体 (动作空间必须是离散的)\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_size = env.action_space.n\n",
    "        Agent = RLAgent(action_size=action_size)\n",
    "    else:\n",
    "        logger.error(\"Action space is not Discrete!\")\n",
    "        raise ValueError(\"Action space is not Discrete!\")\n",
    "\n",
    "    # 循环每个回合\n",
    "    for episode in tqdm(range(num_train_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        current_action = None\n",
    "        \n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "\n",
    "        # 回合中的每一步\n",
    "        for step in range(max_steps):\n",
    "            # 处理当前状态\n",
    "            current_states = Agent.processing_states(frame_buffer)\n",
    "            # 选择动作 & 对数概率\n",
    "            if step % frame_stack == 0:\n",
    "                output = Agent.select_action(current_states)\n",
    "                action, log_prob = output['action'].item(), output['log_prob']\n",
    "                current_action = action\n",
    "                logger.debug(f\"Selected action: {action}\")\n",
    "            # 执行动作\n",
    "            observation, reward, terminated, truncated, info = env.step(current_action)\n",
    "            total_reward += reward\n",
    "            logger.debug(f\"Step {step + 1} | Reward: {reward} | Total Reward: {total_reward} | Terminated: {terminated} | Truncated: {truncated} | Info: {info}\")\n",
    "            # 更新帧缓冲区\n",
    "            frame_buffer.append(observation)\n",
    "            # 保存到记忆区\n",
    "            if step % frame_stack == 0:\n",
    "                Agent.memory_buffer.append({\"St\": current_states, \"At\": current_action, \"Rt\": reward, \"log_prob\": log_prob})\n",
    "            # 如果该动作不需要决策动作, 则叠加奖励\n",
    "            else:\n",
    "                Agent.memory_buffer[-1][\"Rt\"] += reward\n",
    "            # 判断是否结束该回合\n",
    "            if terminated or truncated:\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Episode finish, total step {step + 1} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Episode finish, total step {step + 1} | Total Reward: {total_reward}\")\n",
    "                total_reward = 0\n",
    "                break\n",
    "        \n",
    "        # 更新模型\n",
    "        Agent.update_policy()\n",
    "        \n",
    "        # 保存模型\n",
    "        if (episode + 1) % save_freq == 0 and episode != 0:\n",
    "            episodes = episode + 1\n",
    "            Agent.save_model(episodes)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 评估智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估但不录制视频\n",
    "if is_evaluate == 1 and need_record == 0:\n",
    "    eval_env = gym.make(env_id, render_mode=\"human\")\n",
    "    eval_env = CustomEnv(eval_env)\n",
    "# 评估且需要录制视频\n",
    "elif is_evaluate == 1 and need_record == 1:\n",
    "    eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    eval_env = CustomEnv(eval_env)\n",
    "\n",
    "# 如果启用了评估\n",
    "if is_evaluate == 1:\n",
    "\n",
    "    # 初始化用于评估的参数\n",
    "    frame_record = []\n",
    "    max_reward = 0\n",
    "\n",
    "    # 实例化用于评估的智能体\n",
    "    Agent = RLAgent(action_size=eval_env.action_space.n)\n",
    "\n",
    "    # 每个回合\n",
    "    for episode in tqdm(range(num_eval_episodes)):\n",
    "        # 初始化环境\n",
    "        state, info = eval_env.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        frame_buffer = deque(maxlen=frame_stack)\n",
    "        current_action = None\n",
    "        # 初始化帧缓冲区\n",
    "        for _ in range(frame_stack): \n",
    "            frame_buffer.append(state)\n",
    "            \n",
    "        # 回合中的每一步\n",
    "        for step in range(max_steps):\n",
    "            # 处理当前状态\n",
    "            current_states = Agent.processing_states(frame_buffer)\n",
    "            # 选择动作\n",
    "            if step % frame_stack == 0:\n",
    "                output = Agent.select_action(current_states, sample=eval_sample_action)\n",
    "                current_action = output[\"action\"].item()\n",
    "            # 执行该动作\n",
    "            observation, reward, terminated, truncated, info = eval_env.step(current_action)\n",
    "            total_reward += reward\n",
    "            # 更新帧缓冲区\n",
    "            frame_buffer.append(observation)\n",
    "            # 如果需要记录视频, 则渲染画面 eval_env.render(), 然后将此画面添加到 frame_record 中\n",
    "            if need_record:\n",
    "                frame_record.append(eval_env.render())\n",
    "                \n",
    "            # 判断是否结束\n",
    "            if terminated or truncated:\n",
    "                # 如果需要记录视频, 则保留最好的记录\n",
    "                if need_record and total_reward > max_reward:\n",
    "                    np_frame_record = np.array(frame_record)\n",
    "                    max_reward = total_reward\n",
    "                    frame_record.clear()\n",
    "                # 评估奖励\n",
    "                if total_reward >= reward_threshold:\n",
    "                    logger.success(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Step {step + 1} | Total Reward: {total_reward}\")\n",
    "                break\n",
    "\n",
    "    # 记录评估结果(只记录最好的奖励轮次)\n",
    "    if need_record:\n",
    "        record_file = f\"{os.path.abspath(os.path.join(save_dir, evaluate_record_perfix))}{int(max_reward)}.mp4\"\n",
    "        imageio.mimsave(record_file, np_frame_record, fps=evaluate_record_fps, quality=evaluate_record_quality)\n",
    "        logger.info(f\"The best evaluation record is: {record_file}\")\n",
    "\n",
    "    # 关闭环境\n",
    "    eval_env.close()\n",
    "    pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
